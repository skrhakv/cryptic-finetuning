{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8209e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1365  0.2307  0.2751  0.3447  0.3547  0.565   0.3354  0.751   0.3079\n",
      " 0.6426  0.5156  0.8257  0.942   0.869   0.9253  0.7485  0.801   0.816\n",
      " 0.7183  0.446   0.2441  0.4717  0.4849  0.2822  0.4011  0.3726  0.2496\n",
      " 0.588   0.711   0.752   0.645   0.714   0.7246  0.5537  0.7036  0.6147\n",
      " 0.639   0.61    0.543   0.6655  0.5884  0.564   0.4265  0.2524  0.2012\n",
      " 0.1455  0.2242  0.2009  0.1746  0.1746  0.12103 0.1559  0.2203  0.2106\n",
      " 0.33    0.4504  0.7163  0.767   0.8545  0.772   0.6387  0.8135  0.6265\n",
      " 0.4731  0.4404  0.3525  0.313   0.6807  0.6543  0.3076  0.466   0.75\n",
      " 0.6245  0.4763  0.3684  0.3962  0.3616  0.09686 0.4937  0.12305 0.1226\n",
      " 0.09503 0.1968  0.2615  0.277   0.3704  0.277   0.2325  0.2524  0.1844\n",
      " 0.1643  0.2118  0.2246  0.1383  0.2334  0.34    0.3284  0.2632  0.2534\n",
      " 0.5117  0.3667  0.2603  0.545   0.725   0.6895  0.371   0.302   0.2874\n",
      " 0.2437  0.258   0.2346  0.207   0.11475 0.0863  0.1532  0.10486 0.5933\n",
      " 0.9097  0.5107  0.689   0.724   0.316   0.1787  0.2964  0.2101  0.1542\n",
      " 0.2125  0.1771  0.215   0.2197  0.0695  0.2096  0.2233  0.2273  0.0715\n",
      " 0.2607  0.2128  0.3079  0.1919  0.2622  0.1611  0.1272  0.1711  0.403\n",
      " 0.4802  0.713   0.82    0.8813  0.5347  0.5684  0.4004  0.2388  0.3354\n",
      " 0.2947  0.237   0.07947 0.222   0.2343  0.1437  0.1594  0.124   0.2274\n",
      " 0.2023  0.2998  0.289   0.2208  0.3518  0.3494  0.2195  0.248  ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# some constants\n",
    "MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "MAX_LENGTH = 1024\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "OUTPUT_SIZE = 1\n",
    "DROPOUT = 0.25\n",
    "\n",
    "# UPDATE THIS!\n",
    "MODEL_PATH = '/home/skrhakv/cryptic-nn/src/650M-version/finetuned-model-650M.pt'\n",
    "\n",
    "# define the model - if we do not define the model then the loading of the model will fail\n",
    "class FinetuneESM(nn.Module):\n",
    "    def __init__(self, esm_model: str) -> None:\n",
    "        super().__init__()\n",
    "        self.llm = EsmModel.from_pretrained(esm_model)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE)\n",
    "        self.plDDT_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE)\n",
    "        self.distance_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE)\n",
    "\n",
    "    def forward(self, batch: dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        token_embeddings = self.llm(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        return self.classifier(token_embeddings), self.plDDT_regressor(token_embeddings), self.distance_regressor(token_embeddings)\n",
    "\n",
    "# load the model\n",
    "model = torch.load(MODEL_PATH, weights_only=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "KRAS_sequence = 'GMTEYKLVVVGACGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETSLLDILDTAGQEEYSAMRDQYMRTGEGFLLVFAINNTKSFEDIHHYREQIKRVKDSEDVPMVLVGNKSDLPSRTVDTKQAQDLARSYGIPFIETSAKTRQGVDDAFYTLVREIRKHKEK'\n",
    "\n",
    "# tokenize the sequence\n",
    "tokenized_sequences = tokenizer(KRAS_sequence, max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "tokenized_sequences = {k: torch.tensor([v]).to(DEVICE) for k,v in tokenized_sequences.items()}\n",
    "\n",
    "# predict\n",
    "output, _, _ = model(tokenized_sequences)\n",
    "output = output.flatten()\n",
    "\n",
    "mask = (tokenized_sequences['attention_mask'] == 1).flatten()\n",
    "\n",
    "output = torch.sigmoid(output[mask][1:-1]).detach().cpu().numpy()\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
