{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "DATASET = 'cryptobench'\n",
    "DATA_PATH = f'/home/skrhakv/cryptic-nn/data/{DATASET}'\n",
    "ESM_EMBEDDINGS_PATH = f'{DATA_PATH}/embeddings'\n",
    "ADJACENCY_MATRICES_PATH = f'{DATA_PATH}/distance-matrices' \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_THRESHOLD = 10\n",
    "CLASS_THRESHOLD = 0.75 # threshold set according to the MCC for different threshold, tried below\n",
    "def process_sequence_dataset(annotation_path, embeddings_paths):\n",
    "    Xs = {}\n",
    "    Ys = {}\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "        for row in reader:\n",
    "            id = row[0].lower() + row[1]\n",
    "            sequence = row[4]\n",
    "\n",
    "            if row[3] == '':\n",
    "                continue\n",
    "            \n",
    "            # load the precomputed embedding\n",
    "            if id not in Xs:\n",
    "                for embeddings_path in embeddings_paths:\n",
    "                    filename = id + '.npy'\n",
    "                    embedding = np.load(f'{embeddings_path}/{filename}')\n",
    "                    if id not in Xs:\n",
    "                        Xs[id] = embedding\n",
    "                    else:\n",
    "                        Xs[id] = np.concatenate((Xs[id],embedding), axis = 1)\n",
    "                    \n",
    "\n",
    "            # load the annotations denoting whether particular residue is binding or not\n",
    "            # we use binary annotation: 0=non-binding; 1=binding\n",
    "            if id not in Ys:\n",
    "                Ys[id] = np.zeros(embedding.shape[0])\n",
    "            for (aa, residue_idx) in [(residue[0], int(residue[1:])) for residue in row[3].split(' ')]:\n",
    "                assert sequence[residue_idx] == aa\n",
    "                Ys[id][residue_idx] = 1\n",
    "\n",
    "    return Xs, Ys\n",
    "\n",
    "\n",
    "def get_adjacency_info(id):\n",
    "    distance_matrix = np.load(f'{ADJACENCY_MATRICES_PATH}/{id}.npy')\n",
    "\n",
    "    edge_indices = []\n",
    "\n",
    "    for iy, ix in np.ndindex(distance_matrix.shape):\n",
    "        if iy >= ix:\n",
    "            continue\n",
    "\n",
    "        if distance_matrix[iy, ix] <= DISTANCE_THRESHOLD:\n",
    "            edge_indices += [[iy, ix], [ix, iy]]\n",
    "    \n",
    "    edge_indices = torch.tensor(edge_indices)\n",
    "    edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
    "    return edge_indices\n",
    "\n",
    "\n",
    "def load_dataset(dataset_annotation_filepath):\n",
    "    Xs, Ys = process_sequence_dataset(dataset_annotation_filepath, [ESM_EMBEDDINGS_PATH])\n",
    "\n",
    "    protein_list = []\n",
    "    for key in Xs.keys():\n",
    "        protein_features = torch.tensor(Xs[key], dtype=torch.float32)\n",
    "        protein_labels = torch.tensor(Ys[key], dtype=torch.int64)\n",
    "        protein_edges = get_adjacency_info(key)\n",
    "        protein = Data(x=protein_features, edge_index=protein_edges, y=protein_labels)\n",
    "        protein_list.append(protein)\n",
    "        if protein_edges.shape[1] > 0:\n",
    "            if protein_edges.max() >= protein_features.size(0):\n",
    "                print(f'{key}: {protein_edges.max()}, {protein_features.size(0)}')\n",
    "        \n",
    "    return protein_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proteins = load_dataset(f'{DATA_PATH}/test.txt')\n",
    "train_proteins = load_dataset(f'{DATA_PATH}/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DROPOUT = 0.3\n",
    "EMBEDDING_DIM = 2560\n",
    "HEADS = 16\n",
    "HIDDEN_CHANNELS = 100\n",
    "LAYER_WIDTH = 256\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.dropout0 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.conv1 = GATConv(EMBEDDING_DIM, HIDDEN_CHANNELS, heads=HEADS, dropout=DROPOUT, concat=False)\n",
    "        # self.dropout00 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        # self.conv2 = GATConv(HIDDEN_CHANNELS, HIDDEN_CHANNELS, heads=HEADS,\n",
    "        #                     concat=False, dropout=DROPOUT)\n",
    "\n",
    "        self.linear1 = nn.Linear(HIDDEN_CHANNELS, LAYER_WIDTH)\n",
    "        self.dropout1 = nn.Dropout(DROPOUT)\n",
    "        \n",
    "        self.linear2 = nn.Linear(in_features=LAYER_WIDTH, out_features=LAYER_WIDTH)\n",
    "        self.dropout2 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.linear3 = nn.Linear(in_features=LAYER_WIDTH, out_features=1)\n",
    "        self.dropout3 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.dropout0(x)\n",
    "    \n",
    "        x = self.conv1(x, edge_index)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.dropout00(x)\n",
    "\n",
    "        # x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        x = x.relu()\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = x.relu()\n",
    "        x = self.dropout3(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computed externally:\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def train(model, optimizer, epochs, train_dataloader, test_dataloader):\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # compute class weights (because the dataset is heavily imbalanced)\n",
    "    print(f'Class weights: ', class_weights)\n",
    "    # BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #\n",
    "        # TEST\n",
    "        #\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for batch_id, data in enumerate(test_dataloader):   \n",
    "                X_test = data.x.to(device)\n",
    "                y_test = data.y.to(device).float()\n",
    "                edges = data.edge_index.to(device)\n",
    "\n",
    "                test_logits = model(X_test, edges).squeeze()\n",
    "                test_probs = torch.sigmoid(test_logits)\n",
    "                test_pred = test_probs > CLASS_THRESHOLD\n",
    "\n",
    "                test_loss = loss_fn(test_logits,\n",
    "                                    y_test)\n",
    "                test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "                # compute metrics on test dataset                \n",
    "                test_acc = accuracy_fn(y_true=y_test,\n",
    "                                       y_pred=test_pred)\n",
    "                fpr, tpr, thresholds1 = metrics.roc_curve(y_test.cpu().float().numpy(), torch.sigmoid(test_logits).cpu().float().numpy())\n",
    "                roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "                mcc = metrics.matthews_corrcoef(y_test.cpu().float().numpy(), test_pred.cpu().float().numpy())\n",
    "\n",
    "                f1 = metrics.f1_score(y_test.cpu().float().numpy(), test_pred.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "                precision, recall, thresholds2 = metrics.precision_recall_curve(y_test.cpu().float().numpy(), torch.sigmoid(test_logits).cpu().float().numpy())\n",
    "                auprc = metrics.auc(recall, precision)\n",
    "\n",
    "        #\n",
    "        # TRAIN\n",
    "        #\n",
    "        batch_losses = []\n",
    "        for id_batch, data in enumerate(train_dataloader):\n",
    "            x_batch = data.x.to(device)\n",
    "            y_batch = data.y.to(device).float()\n",
    "            edges = data.edge_index.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_logits = model(x_batch, edges).squeeze()\n",
    "            y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "            loss = loss_fn(y_logits,\n",
    "                           y_batch)\n",
    "            acc = accuracy_fn(y_true=y_batch,\n",
    "                              y_pred=y_pred)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "        #if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(test_pred.to(dtype=torch.int))}\")\n",
    "    np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/GAT-rocauc.npz', fpr, tpr, thresholds1)\n",
    "    np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/GAT-auprc.npz', precision, recall, thresholds2)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights:  tensor([0.5303, 8.7481], device='cuda:0')\n",
      "Epoch: 0 | Loss: 1.30793, Accuracy: 94.32% | Test loss: 1.01670, AUC: 0.4586, MCC: 0.0000, F1: 0.9156, AUPRC: 0.0542, sum: 0\n",
      "Epoch: 1 | Loss: 0.97750, Accuracy: 94.32% | Test loss: 0.80869, AUC: 0.8025, MCC: 0.0000, F1: 0.9156, AUPRC: 0.1971, sum: 0\n",
      "Epoch: 2 | Loss: 0.72584, Accuracy: 94.27% | Test loss: 0.67551, AUC: 0.8418, MCC: 0.1259, F1: 0.9197, AUPRC: 0.2577, sum: 297\n",
      "Epoch: 3 | Loss: 0.57169, Accuracy: 92.27% | Test loss: 0.63372, AUC: 0.8593, MCC: 0.3388, F1: 0.9258, AUPRC: 0.3062, sum: 3752\n",
      "Epoch: 4 | Loss: 0.42447, Accuracy: 92.73% | Test loss: 0.61903, AUC: 0.8658, MCC: 0.3604, F1: 0.9293, AUPRC: 0.3321, sum: 3557\n",
      "Epoch: 5 | Loss: 0.59552, Accuracy: 92.82% | Test loss: 0.61294, AUC: 0.8694, MCC: 0.3726, F1: 0.9304, AUPRC: 0.3497, sum: 3598\n",
      "Epoch: 6 | Loss: 0.61403, Accuracy: 92.65% | Test loss: 0.60633, AUC: 0.8716, MCC: 0.3859, F1: 0.9302, AUPRC: 0.3622, sum: 3910\n",
      "Epoch: 7 | Loss: 0.28696, Accuracy: 92.63% | Test loss: 0.61039, AUC: 0.8704, MCC: 0.3950, F1: 0.9305, AUPRC: 0.3697, sum: 4035\n",
      "Epoch: 8 | Loss: 0.60913, Accuracy: 93.25% | Test loss: 0.60703, AUC: 0.8724, MCC: 0.3897, F1: 0.9335, AUPRC: 0.3738, sum: 3365\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(train_proteins, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_proteins, batch_size=len(test_proteins))\n",
    "\n",
    "model = GAT().to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "train_losses, test_losses = train(model, optimizer, 9, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/home/skrhakv/cryptic-nn/src/models/train-models/GAT.pt'\n",
    "torch.save(model, OUTPUT_PATH)\n",
    "# Epoch: 8 | Loss: 0.60913, Accuracy: 93.25% | Test loss: 0.60703, AUC: 0.8724, MCC: 0.3897, F1: 0.9335, AUPRC: 0.3738, sum: 3365\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.00 | Accuracy: 0.0568 | MCC: 0.0000 | F1: 0.0061\n",
      "Threshold: 0.05 | Accuracy: 0.5859 | MCC: 0.2105 | F1: 0.6913\n",
      "Threshold: 0.10 | Accuracy: 0.6696 | MCC: 0.2402 | F1: 0.7579\n",
      "Threshold: 0.15 | Accuracy: 0.7239 | MCC: 0.2615 | F1: 0.7978\n",
      "Threshold: 0.20 | Accuracy: 0.7627 | MCC: 0.2818 | F1: 0.8249\n",
      "Threshold: 0.25 | Accuracy: 0.7952 | MCC: 0.2955 | F1: 0.8468\n",
      "Threshold: 0.30 | Accuracy: 0.8182 | MCC: 0.3088 | F1: 0.8620\n",
      "Threshold: 0.35 | Accuracy: 0.8382 | MCC: 0.3222 | F1: 0.8751\n",
      "Threshold: 0.40 | Accuracy: 0.8599 | MCC: 0.3358 | F1: 0.8889\n",
      "Threshold: 0.45 | Accuracy: 0.8718 | MCC: 0.3399 | F1: 0.8963\n",
      "Threshold: 0.50 | Accuracy: 0.8868 | MCC: 0.3500 | F1: 0.9057\n",
      "Threshold: 0.55 | Accuracy: 0.8971 | MCC: 0.3499 | F1: 0.9118\n",
      "Threshold: 0.60 | Accuracy: 0.9058 | MCC: 0.3603 | F1: 0.9173\n",
      "Threshold: 0.65 | Accuracy: 0.9142 | MCC: 0.3569 | F1: 0.9220\n",
      "Threshold: 0.70 | Accuracy: 0.9215 | MCC: 0.3551 | F1: 0.9259\n",
      "Threshold: 0.75 | Accuracy: 0.9291 | MCC: 0.3608 | F1: 0.9303\n",
      "Threshold: 0.80 | Accuracy: 0.9337 | MCC: 0.3509 | F1: 0.9320\n",
      "Threshold: 0.85 | Accuracy: 0.9392 | MCC: 0.3477 | F1: 0.9342\n",
      "Threshold: 0.90 | Accuracy: 0.9421 | MCC: 0.3025 | F1: 0.9321\n",
      "Threshold: 0.95 | Accuracy: 0.9450 | MCC: 0.2278 | F1: 0.9260\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "thresholds = np.arange(0.0, 1.0, 0.05)\n",
    "for threshold in thresholds:\n",
    "    with torch.inference_mode():\n",
    "        for batch_id, data in enumerate(test_loader):   \n",
    "            X_test = data.x.to(device)\n",
    "            y_test = data.y.to(device).float()\n",
    "            edges = data.edge_index.to(device)\n",
    "\n",
    "            test_logits = model(X_test, edges).squeeze()\n",
    "            test_probs = torch.sigmoid(test_logits)\n",
    "            rounded_predictions = (test_probs > threshold).cpu().numpy().astype(int)\n",
    "            y_test = data.y.to(device).float().cpu().numpy()\n",
    "\n",
    "            acc = metrics.accuracy_score(y_test, rounded_predictions)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(y_test, rounded_predictions)\n",
    "            f1 = metrics.f1_score(y_test, rounded_predictions, average='weighted')\n",
    "\n",
    "            print(f\"Threshold: {threshold:.2f} | Accuracy: {acc:.4f} | MCC: {mcc:.4f} | F1: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
