{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "DATASET = 'cryptobench'\n",
    "DATA_PATH = f'/home/skrhakv/cryptic-nn/data/{DATASET}'\n",
    "ESM_EMBEDDINGS_PATH = f'{DATA_PATH}/embeddings'\n",
    "ADJACENCY_MATRICES_PATH = f'{DATA_PATH}/distance-matrices' \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_THRESHOLD = 10\n",
    "CLASS_THRESHOLD = 0.75 # threshold set according to the MCC for different threshold, tried below\n",
    "def process_sequence_dataset(annotation_path, embeddings_paths):\n",
    "    Xs = {}\n",
    "    Ys = {}\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "        for row in reader:\n",
    "            id = row[0].lower() + row[1]\n",
    "            sequence = row[4]\n",
    "\n",
    "            if row[3] == '':\n",
    "                continue\n",
    "            \n",
    "            # load the precomputed embedding\n",
    "            if id not in Xs:\n",
    "                for embeddings_path in embeddings_paths:\n",
    "                    filename = id + '.npy'\n",
    "                    embedding = np.load(f'{embeddings_path}/{filename}')\n",
    "                    if id not in Xs:\n",
    "                        Xs[id] = embedding\n",
    "                    else:\n",
    "                        Xs[id] = np.concatenate((Xs[id],embedding), axis = 1)\n",
    "                    \n",
    "\n",
    "            # load the annotations denoting whether particular residue is binding or not\n",
    "            # we use binary annotation: 0=non-binding; 1=binding\n",
    "            if id not in Ys:\n",
    "                Ys[id] = np.zeros(embedding.shape[0])\n",
    "            for (aa, residue_idx) in [(residue[0], int(residue[1:])) for residue in row[3].split(' ')]:\n",
    "                assert sequence[residue_idx] == aa\n",
    "                Ys[id][residue_idx] = 1\n",
    "\n",
    "    return Xs, Ys\n",
    "\n",
    "\n",
    "def get_adjacency_info(id):\n",
    "    distance_matrix = np.load(f'{ADJACENCY_MATRICES_PATH}/{id}.npy')\n",
    "\n",
    "    edge_indices = []\n",
    "\n",
    "    for iy, ix in np.ndindex(distance_matrix.shape):\n",
    "        if iy >= ix:\n",
    "            continue\n",
    "\n",
    "        if distance_matrix[iy, ix] <= DISTANCE_THRESHOLD:\n",
    "            edge_indices += [[iy, ix], [ix, iy]]\n",
    "    \n",
    "    edge_indices = torch.tensor(edge_indices)\n",
    "    edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
    "    return edge_indices\n",
    "\n",
    "\n",
    "def load_dataset(dataset_annotation_filepath):\n",
    "    Xs, Ys = process_sequence_dataset(dataset_annotation_filepath, [ESM_EMBEDDINGS_PATH])\n",
    "\n",
    "    protein_list = []\n",
    "    for key in Xs.keys():\n",
    "        protein_features = torch.tensor(Xs[key], dtype=torch.float32)\n",
    "        protein_labels = torch.tensor(Ys[key], dtype=torch.int64)\n",
    "        protein_edges = get_adjacency_info(key)\n",
    "        protein = Data(x=protein_features, edge_index=protein_edges, y=protein_labels)\n",
    "        protein_list.append(protein)\n",
    "        if protein_edges.shape[1] > 0:\n",
    "            if protein_edges.max() >= protein_features.size(0):\n",
    "                print(f'{key}: {protein_edges.max()}, {protein_features.size(0)}')\n",
    "        \n",
    "    return protein_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proteins = load_dataset(f'{DATA_PATH}/test.txt')\n",
    "train_proteins = load_dataset(f'{DATA_PATH}/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DROPOUT = 0.3\n",
    "EMBEDDING_DIM = 2560\n",
    "HEADS = 16\n",
    "HIDDEN_CHANNELS = 100\n",
    "LAYER_WIDTH = 256\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, number_of_GAT_layers=1):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "\n",
    "        self.GAT_convs = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(DROPOUT) for _ in range(number_of_GAT_layers)])\n",
    "        for i in range(number_of_GAT_layers):\n",
    "            input_dim = EMBEDDING_DIM if i == 0 else HIDDEN_CHANNELS\n",
    "            self.GAT_convs.append(GATConv(input_dim, HIDDEN_CHANNELS, heads=HEADS, dropout=DROPOUT, concat=False))\n",
    "            # self.GAT_convs.append(GCNConv(input_dim, HIDDEN_CHANNELS))\n",
    "\n",
    "        self.linear1 = nn.Linear(HIDDEN_CHANNELS, LAYER_WIDTH)\n",
    "        \n",
    "        self.linear2 = nn.Linear(in_features=LAYER_WIDTH, out_features=LAYER_WIDTH)\n",
    "        self.dropout2 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.linear3 = nn.Linear(in_features=LAYER_WIDTH, out_features=1)\n",
    "        self.dropout3 = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(len(self.GAT_convs)):\n",
    "            x = self.GAT_convs[i](x, edge_index)\n",
    "            x = self.dropouts[i](x)\n",
    "            x = self.relu(x)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        x = x.relu()\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = x.relu()\n",
    "        x = self.dropout3(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computed externally:\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def train(model, optimizer, epochs, train_dataloader, test_dataloader):\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # compute class weights (because the dataset is heavily imbalanced)\n",
    "    print(f'Class weights: ', class_weights)\n",
    "    # BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #\n",
    "        # TEST\n",
    "        #\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for batch_id, data in enumerate(test_dataloader):   \n",
    "                X_test = data.x.to(device)\n",
    "                y_test = data.y.to(device).float()\n",
    "                edges = data.edge_index.to(device)\n",
    "\n",
    "                test_logits = model(X_test, edges).squeeze()\n",
    "                test_probs = torch.sigmoid(test_logits)\n",
    "                test_pred = test_probs > CLASS_THRESHOLD\n",
    "\n",
    "                test_loss = loss_fn(test_logits,\n",
    "                                    y_test)\n",
    "                test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "                # compute metrics on test dataset                \n",
    "                test_acc = accuracy_fn(y_true=y_test,\n",
    "                                       y_pred=test_pred)\n",
    "                fpr, tpr, thresholds1 = metrics.roc_curve(y_test.cpu().float().numpy(), torch.sigmoid(test_logits).cpu().float().numpy())\n",
    "                roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "                mcc = metrics.matthews_corrcoef(y_test.cpu().float().numpy(), test_pred.cpu().float().numpy())\n",
    "\n",
    "                f1 = metrics.f1_score(y_test.cpu().float().numpy(), test_pred.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "                precision, recall, thresholds2 = metrics.precision_recall_curve(y_test.cpu().float().numpy(), torch.sigmoid(test_logits).cpu().float().numpy())\n",
    "                auprc = metrics.auc(recall, precision)\n",
    "\n",
    "        #\n",
    "        # TRAIN\n",
    "        #\n",
    "        batch_losses = []\n",
    "        for id_batch, data in enumerate(train_dataloader):\n",
    "            x_batch = data.x.to(device)\n",
    "            y_batch = data.y.to(device).float()\n",
    "            edges = data.edge_index.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_logits = model(x_batch, edges).squeeze()\n",
    "            y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "            loss = loss_fn(y_logits,\n",
    "                           y_batch)\n",
    "            acc = accuracy_fn(y_true=y_batch,\n",
    "                              y_pred=y_pred)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "        #if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(test_pred.to(dtype=torch.int))}\")\n",
    "    # np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/GAT-rocauc.npz', fpr, tpr, thresholds1)\n",
    "    # np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/GAT-auprc.npz', precision, recall, thresholds2)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training GAT with 1 layers\n",
      "Class weights:  tensor([0.5303, 8.7481], device='cuda:0')\n",
      "Epoch: 0 | Loss: 1.30211, Accuracy: 94.32% | Test loss: 1.01008, AUC: 0.5101, MCC: 0.0000, F1: 0.9156, AUPRC: 0.0644, sum: 0\n",
      "Epoch: 1 | Loss: 1.12630, Accuracy: 94.32% | Test loss: 0.84946, AUC: 0.7682, MCC: 0.0000, F1: 0.9156, AUPRC: 0.1576, sum: 0\n",
      "Epoch: 2 | Loss: 0.50292, Accuracy: 94.32% | Test loss: 0.71608, AUC: 0.8273, MCC: 0.0000, F1: 0.9156, AUPRC: 0.2456, sum: 0\n",
      "Epoch: 3 | Loss: 0.77920, Accuracy: 92.31% | Test loss: 0.66516, AUC: 0.8446, MCC: 0.2888, F1: 0.9234, AUPRC: 0.2755, sum: 3214\n",
      "Epoch: 4 | Loss: 0.60664, Accuracy: 93.20% | Test loss: 0.64405, AUC: 0.8545, MCC: 0.3115, F1: 0.9290, AUPRC: 0.3030, sum: 2614\n",
      "Epoch: 5 | Loss: 0.52721, Accuracy: 92.88% | Test loss: 0.64181, AUC: 0.8557, MCC: 0.3353, F1: 0.9288, AUPRC: 0.3174, sum: 3157\n",
      "Epoch: 6 | Loss: 0.57867, Accuracy: 92.55% | Test loss: 0.63692, AUC: 0.8580, MCC: 0.3443, F1: 0.9276, AUPRC: 0.3208, sum: 3550\n",
      "Epoch: 7 | Loss: 0.48064, Accuracy: 92.18% | Test loss: 0.63685, AUC: 0.8603, MCC: 0.3503, F1: 0.9259, AUPRC: 0.3272, sum: 3958\n",
      "Epoch: 8 | Loss: 0.95900, Accuracy: 92.52% | Test loss: 0.63891, AUC: 0.8598, MCC: 0.3546, F1: 0.9279, AUPRC: 0.3310, sum: 3687\n",
      "\n",
      "\n",
      "Training GAT with 2 layers\n",
      "Class weights:  tensor([0.5303, 8.7481], device='cuda:0')\n",
      "Epoch: 0 | Loss: 1.05491, Accuracy: 94.32% | Test loss: 0.98975, AUC: 0.5582, MCC: 0.0000, F1: 0.9156, AUPRC: 0.0703, sum: 0\n",
      "Epoch: 1 | Loss: 1.13174, Accuracy: 94.32% | Test loss: 0.87883, AUC: 0.7436, MCC: 0.0000, F1: 0.9156, AUPRC: 0.1424, sum: 0\n",
      "Epoch: 2 | Loss: 0.45762, Accuracy: 94.32% | Test loss: 0.75228, AUC: 0.8149, MCC: 0.0000, F1: 0.9156, AUPRC: 0.2198, sum: 0\n",
      "Epoch: 3 | Loss: 0.71364, Accuracy: 94.19% | Test loss: 0.67648, AUC: 0.8385, MCC: 0.1190, F1: 0.9195, AUPRC: 0.2507, sum: 350\n",
      "Epoch: 4 | Loss: 0.59853, Accuracy: 94.10% | Test loss: 0.66370, AUC: 0.8457, MCC: 0.2035, F1: 0.9249, AUPRC: 0.2846, sum: 872\n",
      "Epoch: 5 | Loss: 0.50573, Accuracy: 91.79% | Test loss: 0.65613, AUC: 0.8506, MCC: 0.3141, F1: 0.9220, AUPRC: 0.2841, sum: 3922\n",
      "Epoch: 6 | Loss: 0.68477, Accuracy: 92.53% | Test loss: 0.66112, AUC: 0.8500, MCC: 0.3132, F1: 0.9258, AUPRC: 0.2963, sum: 3254\n",
      "Epoch: 7 | Loss: 0.80341, Accuracy: 90.60% | Test loss: 0.65718, AUC: 0.8511, MCC: 0.3255, F1: 0.9160, AUPRC: 0.2952, sum: 5041\n",
      "Epoch: 8 | Loss: 0.48040, Accuracy: 90.67% | Test loss: 0.65699, AUC: 0.8516, MCC: 0.3233, F1: 0.9162, AUPRC: 0.2981, sum: 4967\n",
      "\n",
      "\n",
      "Training GAT with 3 layers\n",
      "Class weights:  tensor([0.5303, 8.7481], device='cuda:0')\n",
      "Epoch: 0 | Loss: 1.13319, Accuracy: 94.32% | Test loss: 1.00998, AUC: 0.5234, MCC: 0.0000, F1: 0.9156, AUPRC: 0.0613, sum: 0\n",
      "Epoch: 1 | Loss: 0.82859, Accuracy: 94.32% | Test loss: 0.90450, AUC: 0.6835, MCC: 0.0000, F1: 0.9156, AUPRC: 0.1103, sum: 0\n",
      "Epoch: 2 | Loss: 0.91716, Accuracy: 94.32% | Test loss: 0.79884, AUC: 0.7781, MCC: 0.0000, F1: 0.9156, AUPRC: 0.1611, sum: 0\n",
      "Epoch: 3 | Loss: 0.79000, Accuracy: 94.31% | Test loss: 0.73879, AUC: 0.8195, MCC: -0.0018, F1: 0.9156, AUPRC: 0.2062, sum: 3\n",
      "Epoch: 4 | Loss: 1.02430, Accuracy: 93.88% | Test loss: 0.69585, AUC: 0.8318, MCC: 0.1555, F1: 0.9214, AUPRC: 0.2321, sum: 821\n",
      "Epoch: 5 | Loss: 0.56305, Accuracy: 91.70% | Test loss: 0.69212, AUC: 0.8341, MCC: 0.2678, F1: 0.9192, AUPRC: 0.2408, sum: 3542\n",
      "Epoch: 6 | Loss: 0.52673, Accuracy: 91.97% | Test loss: 0.68649, AUC: 0.8355, MCC: 0.2705, F1: 0.9207, AUPRC: 0.2491, sum: 3341\n",
      "Epoch: 7 | Loss: 0.78702, Accuracy: 91.72% | Test loss: 0.70405, AUC: 0.8303, MCC: 0.2766, F1: 0.9198, AUPRC: 0.2488, sum: 3609\n",
      "Epoch: 8 | Loss: 0.59577, Accuracy: 93.19% | Test loss: 0.72464, AUC: 0.8293, MCC: 0.2595, F1: 0.9259, AUPRC: 0.2538, sum: 2160\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(train_proteins, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_proteins, batch_size=len(test_proteins))\n",
    "\n",
    "for i in range(1, 4):\n",
    "    print(f'\\n\\nTraining GAT with {i} layers')\n",
    "    model = GAT(i).to(device)\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                                lr=0.0001)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "    train_losses, test_losses = train(model, optimizer, 9, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/home/skrhakv/cryptic-nn/src/models/train-models/GAT.pt'\n",
    "torch.save(model, OUTPUT_PATH)\n",
    "# Epoch: 8 | Loss: 0.60913, Accuracy: 93.25% | Test loss: 0.60703, AUC: 0.8724, MCC: 0.3897, F1: 0.9335, AUPRC: 0.3738, sum: 3365\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.00 | Accuracy: 0.0568 | MCC: 0.0000 | F1: 0.0061\n",
      "Threshold: 0.05 | Accuracy: 0.5859 | MCC: 0.2105 | F1: 0.6913\n",
      "Threshold: 0.10 | Accuracy: 0.6696 | MCC: 0.2402 | F1: 0.7579\n",
      "Threshold: 0.15 | Accuracy: 0.7239 | MCC: 0.2615 | F1: 0.7978\n",
      "Threshold: 0.20 | Accuracy: 0.7627 | MCC: 0.2818 | F1: 0.8249\n",
      "Threshold: 0.25 | Accuracy: 0.7952 | MCC: 0.2955 | F1: 0.8468\n",
      "Threshold: 0.30 | Accuracy: 0.8182 | MCC: 0.3088 | F1: 0.8620\n",
      "Threshold: 0.35 | Accuracy: 0.8382 | MCC: 0.3222 | F1: 0.8751\n",
      "Threshold: 0.40 | Accuracy: 0.8599 | MCC: 0.3358 | F1: 0.8889\n",
      "Threshold: 0.45 | Accuracy: 0.8718 | MCC: 0.3399 | F1: 0.8963\n",
      "Threshold: 0.50 | Accuracy: 0.8868 | MCC: 0.3500 | F1: 0.9057\n",
      "Threshold: 0.55 | Accuracy: 0.8971 | MCC: 0.3499 | F1: 0.9118\n",
      "Threshold: 0.60 | Accuracy: 0.9058 | MCC: 0.3603 | F1: 0.9173\n",
      "Threshold: 0.65 | Accuracy: 0.9142 | MCC: 0.3569 | F1: 0.9220\n",
      "Threshold: 0.70 | Accuracy: 0.9215 | MCC: 0.3551 | F1: 0.9259\n",
      "Threshold: 0.75 | Accuracy: 0.9291 | MCC: 0.3608 | F1: 0.9303\n",
      "Threshold: 0.80 | Accuracy: 0.9337 | MCC: 0.3509 | F1: 0.9320\n",
      "Threshold: 0.85 | Accuracy: 0.9392 | MCC: 0.3477 | F1: 0.9342\n",
      "Threshold: 0.90 | Accuracy: 0.9421 | MCC: 0.3025 | F1: 0.9321\n",
      "Threshold: 0.95 | Accuracy: 0.9450 | MCC: 0.2278 | F1: 0.9260\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "thresholds = np.arange(0.0, 1.0, 0.05)\n",
    "for threshold in thresholds:\n",
    "    with torch.inference_mode():\n",
    "        for batch_id, data in enumerate(test_loader):   \n",
    "            X_test = data.x.to(device)\n",
    "            y_test = data.y.to(device).float()\n",
    "            edges = data.edge_index.to(device)\n",
    "\n",
    "            test_logits = model(X_test, edges).squeeze()\n",
    "            test_probs = torch.sigmoid(test_logits)\n",
    "            rounded_predictions = (test_probs > threshold).cpu().numpy().astype(int)\n",
    "            y_test = data.y.to(device).float().cpu().numpy()\n",
    "\n",
    "            acc = metrics.accuracy_score(y_test, rounded_predictions)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(y_test, rounded_predictions)\n",
    "            f1 = metrics.f1_score(y_test, rounded_predictions, average='weighted')\n",
    "\n",
    "            print(f\"Threshold: {threshold:.2f} | Accuracy: {acc:.4f} | MCC: {mcc:.4f} | F1: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
