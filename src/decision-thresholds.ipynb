{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23f6d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import functools\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/skrhakv/cryptic-nn/src/models')\n",
    "import baseline_utils\n",
    "import finetuning_utils\n",
    "from finetuning_utils import FinetunedEsmModel, MultitaskFinetunedEsmModel, MultitaskFinetunedEsmModelWithCnn\n",
    "\n",
    "MODEL_NAME = 'facebook/esm2_t36_3B_UR50D'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40004e09",
   "metadata": {},
   "source": [
    "## base finetuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d6283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThreshold: 0.10 | Accuracy: 0.3888 | MCC: 0.1554 | F1: 0.5012\n",
      "\tThreshold: 0.15 | Accuracy: 0.5491 | MCC: 0.2133 | F1: 0.6592\n",
      "\tThreshold: 0.20 | Accuracy: 0.6447 | MCC: 0.2536 | F1: 0.7386\n",
      "\tThreshold: 0.25 | Accuracy: 0.7153 | MCC: 0.2895 | F1: 0.7917\n",
      "\tThreshold: 0.30 | Accuracy: 0.7663 | MCC: 0.3197 | F1: 0.8278\n",
      "\tThreshold: 0.35 | Accuracy: 0.8065 | MCC: 0.3452 | F1: 0.8551\n",
      "\tThreshold: 0.40 | Accuracy: 0.8400 | MCC: 0.3720 | F1: 0.8774\n",
      "\tThreshold: 0.45 | Accuracy: 0.8663 | MCC: 0.3937 | F1: 0.8945\n",
      "\tThreshold: 0.50 | Accuracy: 0.8886 | MCC: 0.4170 | F1: 0.9090\n",
      "\tThreshold: 0.55 | Accuracy: 0.9050 | MCC: 0.4320 | F1: 0.9196\n",
      "\tThreshold: 0.60 | Accuracy: 0.9180 | MCC: 0.4423 | F1: 0.9277\n",
      "\tThreshold: 0.65 | Accuracy: 0.9282 | MCC: 0.4477 | F1: 0.9339\n",
      "\tThreshold: 0.70 | Accuracy: 0.9361 | MCC: 0.4435 | F1: 0.9382\n",
      "\tThreshold: 0.75 | Accuracy: 0.9425 | MCC: 0.4387 | F1: 0.9411\n",
      "\tThreshold: 0.80 | Accuracy: 0.9465 | MCC: 0.4127 | F1: 0.9411\n",
      "\tThreshold: 0.85 | Accuracy: 0.9481 | MCC: 0.3669 | F1: 0.9378\n",
      "\tThreshold: 0.90 | Accuracy: 0.9476 | MCC: 0.2949 | F1: 0.9307\n",
      "Best threshold: 0.65:\n",
      "Accuracy: 92.82% | AUC: 0.8876, MCC: 0.4477, F1: 0.9339, AUPRC: 0.4456, sum: 4476\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'base-finetuned-model'\n",
    "MODEL_PATH = f'/home/skrhakv/cryptic-nn/src/models/train-models/{MODEL}.pt'\n",
    "finetuned_model = torch.load(MODEL_PATH, weights_only=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer)\n",
    "val_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "\n",
    "partial_collate_fn = functools.partial(finetuning_utils.collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "\n",
    "        output = finetuned_model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        \n",
    "        labels = valid_flattened_labels.cpu().float().numpy()\n",
    "        predictions = torch.sigmoid(cbs_logits).cpu().float().detach().numpy()\n",
    "        best_threshold, previous_mcc = 0.0, -100\n",
    "        for threshold in np.arange(0.1, 0.95, 0.05):\n",
    "            rounded_predictions = (predictions > threshold).astype(int)\n",
    "            acc = metrics.accuracy_score(labels, rounded_predictions)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(labels, rounded_predictions)\n",
    "            if mcc > previous_mcc:\n",
    "                previous_mcc = mcc\n",
    "                best_threshold = threshold\n",
    "            f1 = metrics.f1_score(labels, rounded_predictions, average='weighted')\n",
    "\n",
    "            print(f\"\\tThreshold: {threshold:.2f} | Accuracy: {acc:.4f} | MCC: {mcc:.4f} | F1: {f1:.4f}\")\n",
    "        predictions = (torch.sigmoid(cbs_logits)>best_threshold).float() # torch.round(torch.sigmoid(cbs_logits))\n",
    "\n",
    "        # compute metrics on test dataset\n",
    "        test_acc = baseline_utils.accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                y_pred=predictions)\n",
    "\n",
    "        fpr, tpr, thresholds1 = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "\n",
    "        f1 = metrics.f1_score(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "        precision, recall, thresholds2 = metrics.precision_recall_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.2f}:\")\n",
    "print(f\"Accuracy: {test_acc:.2f}% | AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(predictions.to(dtype=torch.int))}\")\n",
    "\n",
    "np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/{MODEL}-rocauc.npz', fpr, tpr, thresholds1)\n",
    "np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/{MODEL}-auprc.npz', precision, recall, thresholds2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e9b4d",
   "metadata": {},
   "source": [
    "## multitask model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17435515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThreshold: 0.10 | Accuracy: 0.5195 | MCC: 0.2030 | F1: 0.6326\n",
      "\tThreshold: 0.15 | Accuracy: 0.6391 | MCC: 0.2519 | F1: 0.7342\n",
      "\tThreshold: 0.20 | Accuracy: 0.7085 | MCC: 0.2870 | F1: 0.7868\n",
      "\tThreshold: 0.25 | Accuracy: 0.7585 | MCC: 0.3164 | F1: 0.8224\n",
      "\tThreshold: 0.30 | Accuracy: 0.7968 | MCC: 0.3397 | F1: 0.8486\n",
      "\tThreshold: 0.35 | Accuracy: 0.8298 | MCC: 0.3666 | F1: 0.8708\n",
      "\tThreshold: 0.40 | Accuracy: 0.8559 | MCC: 0.3889 | F1: 0.8879\n",
      "\tThreshold: 0.45 | Accuracy: 0.8769 | MCC: 0.4081 | F1: 0.9015\n",
      "\tThreshold: 0.50 | Accuracy: 0.8931 | MCC: 0.4261 | F1: 0.9122\n",
      "\tThreshold: 0.55 | Accuracy: 0.9059 | MCC: 0.4333 | F1: 0.9202\n",
      "\tThreshold: 0.60 | Accuracy: 0.9173 | MCC: 0.4405 | F1: 0.9273\n",
      "\tThreshold: 0.65 | Accuracy: 0.9266 | MCC: 0.4459 | F1: 0.9329\n",
      "\tThreshold: 0.70 | Accuracy: 0.9339 | MCC: 0.4431 | F1: 0.9369\n",
      "\tThreshold: 0.75 | Accuracy: 0.9400 | MCC: 0.4315 | F1: 0.9395\n",
      "\tThreshold: 0.80 | Accuracy: 0.9448 | MCC: 0.4186 | F1: 0.9409\n",
      "\tThreshold: 0.85 | Accuracy: 0.9475 | MCC: 0.3823 | F1: 0.9390\n",
      "\tThreshold: 0.90 | Accuracy: 0.9478 | MCC: 0.3099 | F1: 0.9323\n",
      "Best threshold: 0.65:\n",
      "Accuracy: 92.66% | AUC: 0.8885, MCC: 0.4459, F1: 0.9329, AUPRC: 0.4397, sum: 4613\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'multitask-finetuned-model'\n",
    "MODEL_PATH = f'/home/skrhakv/cryptic-nn/src/models/train-models/{MODEL}.pt'\n",
    "\n",
    "finetuned_model = torch.load(MODEL_PATH, weights_only=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "plDDT_path = '/home/skrhakv/cryptic-nn/data/ligysis/plDDT'\n",
    "plDDT_scaler = finetuning_utils.train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', plDDT_path=plDDT_path, uniprot_ids=True)\n",
    "val_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "\n",
    "partial_collate_fn = functools.partial(finetuning_utils.collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        output1, _, _ = finetuned_model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        labels = valid_flattened_labels.cpu().float().numpy()\n",
    "        predictions = torch.sigmoid(cbs_logits).cpu().float().detach().numpy()\n",
    "        best_threshold, previous_mcc = 0.0, -100\n",
    "        for threshold in np.arange(0.1, 0.95, 0.05):\n",
    "            rounded_predictions = (predictions > threshold).astype(int)\n",
    "            acc = metrics.accuracy_score(labels, rounded_predictions)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(labels, rounded_predictions)\n",
    "            if mcc > previous_mcc:\n",
    "                previous_mcc = mcc\n",
    "                best_threshold = threshold\n",
    "            f1 = metrics.f1_score(labels, rounded_predictions, average='weighted')\n",
    "\n",
    "            print(f\"\\tThreshold: {threshold:.2f} | Accuracy: {acc:.4f} | MCC: {mcc:.4f} | F1: {f1:.4f}\")\n",
    "        predictions = (torch.sigmoid(cbs_logits)>best_threshold).float() # torch.round(torch.sigmoid(cbs_logits))\n",
    "\n",
    "        # compute metrics on test dataset\n",
    "        test_acc = baseline_utils.accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                y_pred=predictions)\n",
    "        \n",
    "        fpr, tpr, thresholds1 = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "\n",
    "        f1 = metrics.f1_score(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "        precision, recall, thresholds2 = metrics.precision_recall_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.2f}:\")\n",
    "print(f\"Accuracy: {test_acc:.2f}% | AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(predictions.to(dtype=torch.int))}\")\n",
    "\n",
    "np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/{MODEL}-rocauc.npz', fpr, tpr, thresholds1)\n",
    "np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/{MODEL}-auprc.npz', precision, recall, thresholds2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a26faa9",
   "metadata": {},
   "source": [
    "## multitask model with additional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3774098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThreshold: 0.10 | Accuracy: 0.6570 | MCC: 0.2626 | F1: 0.7481\n",
      "\tThreshold: 0.15 | Accuracy: 0.7490 | MCC: 0.3156 | F1: 0.8158\n",
      "\tThreshold: 0.20 | Accuracy: 0.7962 | MCC: 0.3477 | F1: 0.8483\n",
      "\tThreshold: 0.25 | Accuracy: 0.8267 | MCC: 0.3700 | F1: 0.8688\n",
      "\tThreshold: 0.30 | Accuracy: 0.8499 | MCC: 0.3904 | F1: 0.8842\n",
      "\tThreshold: 0.35 | Accuracy: 0.8672 | MCC: 0.4091 | F1: 0.8955\n",
      "\tThreshold: 0.40 | Accuracy: 0.8816 | MCC: 0.4237 | F1: 0.9050\n",
      "\tThreshold: 0.45 | Accuracy: 0.8941 | MCC: 0.4401 | F1: 0.9132\n",
      "\tThreshold: 0.50 | Accuracy: 0.9046 | MCC: 0.4513 | F1: 0.9200\n",
      "\tThreshold: 0.55 | Accuracy: 0.9133 | MCC: 0.4637 | F1: 0.9258\n",
      "\tThreshold: 0.60 | Accuracy: 0.9220 | MCC: 0.4777 | F1: 0.9315\n",
      "\tThreshold: 0.65 | Accuracy: 0.9294 | MCC: 0.4871 | F1: 0.9363\n",
      "\tThreshold: 0.70 | Accuracy: 0.9356 | MCC: 0.4895 | F1: 0.9400\n",
      "\tThreshold: 0.75 | Accuracy: 0.9407 | MCC: 0.4878 | F1: 0.9428\n",
      "\tThreshold: 0.80 | Accuracy: 0.9462 | MCC: 0.4856 | F1: 0.9455\n",
      "\tThreshold: 0.85 | Accuracy: 0.9492 | MCC: 0.4574 | F1: 0.9450\n",
      "\tThreshold: 0.90 | Accuracy: 0.9506 | MCC: 0.4038 | F1: 0.9409\n",
      "Best threshold: 0.70:\n",
      "Accuracy: 93.57% | AUC: 0.8962, MCC: 0.4897, F1: 0.9401, AUPRC: 0.4911, sum: 4247\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'multitask-finetuned-model-with-ligysis'\n",
    "MODEL_PATH = f'/home/skrhakv/cryptic-nn/src/models/train-models/{MODEL}.pt'\n",
    "\n",
    "finetuned_model = torch.load(MODEL_PATH, weights_only=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "plDDT_path = '/home/skrhakv/cryptic-nn/data/ligysis/plDDT'\n",
    "plDDT_scaler = finetuning_utils.train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', plDDT_path=plDDT_path, uniprot_ids=True)\n",
    "val_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "\n",
    "partial_collate_fn = functools.partial(finetuning_utils.collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        output1, _, _ = finetuned_model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        labels = valid_flattened_labels.cpu().float().numpy()\n",
    "        predictions = torch.sigmoid(cbs_logits).cpu().float().detach().numpy()\n",
    "        best_threshold, previous_mcc = 0.0, -100\n",
    "        for threshold in np.arange(0.1, 0.95, 0.05):\n",
    "            rounded_predictions = (predictions > threshold).astype(int)\n",
    "            acc = metrics.accuracy_score(labels, rounded_predictions)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(labels, rounded_predictions)\n",
    "            if mcc > previous_mcc:\n",
    "                previous_mcc = mcc\n",
    "                best_threshold = threshold\n",
    "            f1 = metrics.f1_score(labels, rounded_predictions, average='weighted')\n",
    "\n",
    "            print(f\"\\tThreshold: {threshold:.2f} | Accuracy: {acc:.4f} | MCC: {mcc:.4f} | F1: {f1:.4f}\")\n",
    "        predictions = (torch.sigmoid(cbs_logits)>best_threshold).float() # torch.round(torch.sigmoid(cbs_logits))\n",
    "\n",
    "        # compute metrics on test dataset\n",
    "        test_acc = baseline_utils.accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                y_pred=predictions)\n",
    "\n",
    "        fpr, tpr, thresholds1 = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "\n",
    "        f1 = metrics.f1_score(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "        precision, recall, thresholds2 = metrics.precision_recall_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.2f}:\")\n",
    "print(f\"Accuracy: {test_acc:.2f}% | AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(predictions.to(dtype=torch.int))}\")\n",
    "\n",
    "np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/{MODEL}-rocauc.npz', fpr, tpr, thresholds1)\n",
    "np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/{MODEL}-auprc.npz', precision, recall, thresholds2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd85f5b",
   "metadata": {},
   "source": [
    "## multitask model with additional data and CNN extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919af49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThreshold: 0.10 | Accuracy: 0.2768 | MCC: 0.1158 | F1: 0.3658\n",
      "\tThreshold: 0.15 | Accuracy: 0.4261 | MCC: 0.1666 | F1: 0.5414\n",
      "\tThreshold: 0.20 | Accuracy: 0.5245 | MCC: 0.2035 | F1: 0.6373\n",
      "\tThreshold: 0.25 | Accuracy: 0.5909 | MCC: 0.2292 | F1: 0.6952\n",
      "\tThreshold: 0.30 | Accuracy: 0.6417 | MCC: 0.2519 | F1: 0.7362\n",
      "\tThreshold: 0.35 | Accuracy: 0.6822 | MCC: 0.2732 | F1: 0.7673\n",
      "\tThreshold: 0.40 | Accuracy: 0.7178 | MCC: 0.2953 | F1: 0.7935\n",
      "\tThreshold: 0.45 | Accuracy: 0.7483 | MCC: 0.3126 | F1: 0.8153\n",
      "\tThreshold: 0.50 | Accuracy: 0.7774 | MCC: 0.3338 | F1: 0.8356\n",
      "\tThreshold: 0.55 | Accuracy: 0.8035 | MCC: 0.3524 | F1: 0.8533\n",
      "\tThreshold: 0.60 | Accuracy: 0.8277 | MCC: 0.3721 | F1: 0.8695\n",
      "\tThreshold: 0.65 | Accuracy: 0.8518 | MCC: 0.3941 | F1: 0.8854\n",
      "\tThreshold: 0.70 | Accuracy: 0.8731 | MCC: 0.4135 | F1: 0.8994\n",
      "\tThreshold: 0.75 | Accuracy: 0.8935 | MCC: 0.4343 | F1: 0.9126\n",
      "\tThreshold: 0.80 | Accuracy: 0.9112 | MCC: 0.4531 | F1: 0.9241\n",
      "\tThreshold: 0.85 | Accuracy: 0.9276 | MCC: 0.4664 | F1: 0.9344\n",
      "\tThreshold: 0.90 | Accuracy: 0.9410 | MCC: 0.4606 | F1: 0.9416\n",
      "Best threshold: 0.85:\n",
      "Accuracy: 92.76% | AUC: 0.8907, MCC: 0.4660, F1: 0.9344, AUPRC: 0.4709, sum: 4763\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'multitask-finetuned-model-with-CNN-with-ligysis'\n",
    "MODEL_PATH = f'/home/skrhakv/cryptic-nn/src/models/train-models/{MODEL}.pt'\n",
    "\n",
    "finetuned_model = torch.load(MODEL_PATH, weights_only=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "plDDT_path = '/home/skrhakv/cryptic-nn/data/ligysis/plDDT'\n",
    "plDDT_scaler = finetuning_utils.train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', plDDT_path=plDDT_path, uniprot_ids=True)\n",
    "val_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, plDDT_path='/home/skrhakv/cryptic-nn/data/cryptobench/plDDT', plDDT_scaler=plDDT_scaler)\n",
    "\n",
    "partial_collate_fn = functools.partial(finetuning_utils.collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        output1, _, _ = finetuned_model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        labels = valid_flattened_labels.cpu().float().numpy()\n",
    "        predictions = torch.sigmoid(cbs_logits).cpu().float().detach().numpy()\n",
    "        best_threshold, previous_mcc = 0.0, -100\n",
    "        for threshold in np.arange(0.1, 0.95, 0.05):\n",
    "            rounded_predictions = (predictions > threshold).astype(int)\n",
    "            acc = metrics.accuracy_score(labels, rounded_predictions)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(labels, rounded_predictions)\n",
    "            if mcc > previous_mcc:\n",
    "                previous_mcc = mcc\n",
    "                best_threshold = threshold\n",
    "            f1 = metrics.f1_score(labels, rounded_predictions, average='weighted')\n",
    "\n",
    "            print(f\"\\tThreshold: {threshold:.2f} | Accuracy: {acc:.4f} | MCC: {mcc:.4f} | F1: {f1:.4f}\")\n",
    "        \n",
    "        \n",
    "        predictions = (torch.sigmoid(cbs_logits)>best_threshold).float() # torch.round(torch.sigmoid(cbs_logits))\n",
    "        test_acc = baseline_utils.accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                y_pred=predictions)\n",
    "        fpr, tpr, thresholds1 = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "\n",
    "        f1 = metrics.f1_score(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "        precision, recall, thresholds2 = metrics.precision_recall_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.2f}:\")\n",
    "print(f\"Accuracy: {test_acc:.2f}% | AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(predictions.to(dtype=torch.int))}\")\n",
    "np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/{MODEL}-rocauc.npz', fpr, tpr, thresholds1)\n",
    "np.savez(f'/home/skrhakv/cryptic-nn/src/models/auc-auprc/data/{MODEL}-auprc.npz', precision, recall, thresholds2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
