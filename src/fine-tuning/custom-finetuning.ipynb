{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.esm.modeling_esm import EsmPreTrainedModel, EsmModel\n",
    "import torch.nn as nn\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "LOSS = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "DROPOUT = 0.5\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "class EsmForTokenClassificationCustom(EsmPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.esm = EsmModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(config.hidden_size, OUTPUT_SIZE)\n",
    "        self.distance_regressor = nn.Linear(config.hidden_size, OUTPUT_SIZE)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.esm(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        cbs_logits = self.classifier(sequence_output)\n",
    "        distance_logits = self.distance_regressor(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        # changed to ignore special tokens at the seq start and end \n",
    "        # as well as invalid positions (labels -100)\n",
    "        if labels is not None:\n",
    "            loss_fct = LOSS\n",
    "\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, self.num_labels)\n",
    "\n",
    "            active_labels = torch.where(\n",
    "              active_loss, labels.view(-1), torch.tensor(-100).type_as(labels)\n",
    "            )\n",
    "\n",
    "            valid_logits=active_logits[active_labels!=-100]\n",
    "            valid_labels=active_labels[active_labels!=-100]\n",
    "            \n",
    "            \n",
    "            valid_labels=valid_labels.type(torch.LongTensor).to('cuda:0')\n",
    "            \n",
    "            valid_logits = torch.softmax(valid_logits, dim=1)\n",
    "            loss = loss_fct(valid_logits, valid_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cbs_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )# , TokenClassifierOutput(\n",
    "        #     loss=loss,\n",
    "        #     logits=distance_logits,\n",
    "        #     hidden_states=outputs.hidden_states,\n",
    "        #     attentions=outputs.attentions,\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 12:10:16.498165: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-20 12:10:16.533590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-20 12:10:17.418777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.data.data_collator import DataCollatorMixin\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy\n",
    "# based on transformers DataCollatorForTokenClassification\n",
    "@dataclass\n",
    "class DataCollatorForTokenClassificationESM(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
    "            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def torch_call(self, features):\n",
    "        import torch\n",
    "        if \"distances\" in features[0].keys():\n",
    "            label_names = ['labels', 'distances']\n",
    "        else:\n",
    "            label_names = ['labels']\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = {label_name: [feature[label_name] for feature in features] for label_name in label_names}\n",
    "\n",
    "        no_labels_features = [{k: v for k, v in feature.items() if k not in label_names} for feature in features]\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            no_labels_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = batch[\"input_ids\"].shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "\n",
    "        def to_list(tensor_or_iterable):\n",
    "            if isinstance(tensor_or_iterable, torch.Tensor):\n",
    "                return tensor_or_iterable.tolist()\n",
    "            return list(tensor_or_iterable)\n",
    "\n",
    "        for label_name in label_names:\n",
    "            if padding_side == \"right\":\n",
    "                batch[label_name] = [\n",
    "                    # to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "                    # changed to pad the special tokens at the beginning and end of the sequence\n",
    "                    [self.label_pad_token_id] + to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label)-1) for label in labels[label_name]\n",
    "                ]\n",
    "            else:\n",
    "                batch[label_name] = [\n",
    "                    [self.label_pad_token_id] * (sequence_length - len(label)) + to_list(label) for label in labels[label_name]\n",
    "                ]\n",
    "    \n",
    "            batch[label_name] = torch.tensor(batch[label_name], dtype=torch.float)\n",
    "        return batch\n",
    "\n",
    "def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    length_of_first = examples[0].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "def tolist(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    elif hasattr(x, \"numpy\"):  # Checks for TF tensors without needing the import\n",
    "        x = x.numpy()\n",
    "    return x.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of EsmForTokenClassificationCustom were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight', 'distance_regressor.bias', 'distance_regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "# MODEL_NAME = 'facebook/esm2_t30_150M_UR50D'\n",
    "# MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
    "\n",
    "model = EsmForTokenClassificationCustom.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "def get_dataset(annotation_path, tokenizer):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "\n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "\n",
    "            assert len(sequence) == len(label)\n",
    "\n",
    "    train_tokenized = tokenizer(sequences, max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = get_dataset('/home/skrhakv/cryptic-nn/src/fine-tuning/train.txt', tokenizer)\n",
    "val_dataset = get_dataset('/home/skrhakv/cryptic-nn/src/fine-tuning/val.txt', tokenizer)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassificationESM(tokenizer) \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, shuffle=True, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.73659, Accuracy: 17.84% | Test loss: 1.04761, AUC: 0.4395135999006268, MCC: -0.053494933235651576, sum: 48365.0\n",
      "Epoch: 1 | Loss: 0.37189, Accuracy: 86.20% | Test loss: 0.60297, AUC: 0.8741129887987007, MCC: 0.3633713411500189, sum: 9149.0\n",
      "Epoch: 2 | Loss: 0.18138, Accuracy: 89.32% | Test loss: 0.61765, AUC: 0.8726657087875005, MCC: 0.3940537278837092, sum: 7042.0\n",
      "Epoch: 3 | Loss: 0.24919, Accuracy: 90.79% | Test loss: 0.79896, AUC: 0.8621024539783995, MCC: 0.3896582797271626, sum: 5727.0\n",
      "Epoch: 4 | Loss: 0.07897, Accuracy: 90.17% | Test loss: 0.81526, AUC: 0.8614779733187512, MCC: 0.38747794123876006, sum: 6235.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m     94\u001b[0m logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m flattened_labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# print('after train prediction')\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# print_used_memory()\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def print_used_memory():\n",
    "    free, total = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
    "    mem_used_MB = (total - free) / 1024 ** 2\n",
    "    mem_total_MB = (total) / 1024 ** 2\n",
    "    print(f'{mem_used_MB} MB / {mem_total_MB} MB')\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "        \n",
    "            logits = output.logits.flatten(1)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            del input_ids, attention_mask, labels, logits, valid_flattened_logits, valid_flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        # print('before train prediction')\n",
    "        # print_used_memory()\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits.flatten(1)\n",
    "        labels = batch['labels'].to(device)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print('after train prediction')\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 35M:\n",
    "# Epoch: 3 | Loss: 0.26539, Accuracy: 87.91% | Test loss: 0.71823, AUC: 0.8429591457364806, MCC: 0.33372585153627116, sum: 7430.0\n",
    "\n",
    "# 150M:\n",
    "# Epoch: 2 | Loss: 0.47842, Accuracy: 89.44% | Test loss: 0.63896, AUC: 0.8584368023837566, MCC: 0.36990818473863774, sum: 6639.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def print_used_memory():\n",
    "    free, total = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
    "    mem_used_MB = (total - free) / 1024 ** 2\n",
    "    mem_total_MB = (total) / 1024 ** 2\n",
    "    print(f'{mem_used_MB} MB / {mem_total_MB} MB')\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    print('Before test:')\n",
    "    print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            print('before prediction:')\n",
    "\n",
    "            print_used_memory()\n",
    "\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            print('after prediction:')\n",
    "            print_used_memory()\n",
    "        \n",
    "            logits = output.logits.flatten(1)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss =  loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            del input_ids, attention_mask, labels, logits, valid_flattened_logits, valid_flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print('after test')\n",
    "    print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        print('before train prediction')\n",
    "        print_used_memory()\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits.flatten(1)\n",
    "        labels = batch['labels'].to(device)\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 35M:\n",
    "# Epoch: 3 | Loss: 0.26539, Accuracy: 87.91% | Test loss: 0.71823, AUC: 0.8429591457364806, MCC: 0.33372585153627116, sum: 7430.0\n",
    "\n",
    "# 150M:\n",
    "# Epoch: 2 | Loss: 0.47842, Accuracy: 89.44% | Test loss: 0.63896, AUC: 0.8584368023837566, MCC: 0.36990818473863774, sum: 6639.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before test:\n",
    "3043.0 MB / 81153.75 MB\n",
    "before prediction:\n",
    "3043.0 MB / 81153.75 MB\n",
    "after prediction:\n",
    "31835.0 MB / 81153.75 MB\n",
    "after test\n",
    "31837.0 MB / 81153.75 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8M:\n",
    "\n",
    "Epoch: 0 | Loss: 0.78285, Accuracy: 18.43% | Test loss: 1.03734, AUC: 0.42720166912931357, MCC: -0.054964089541398004, sum: 47960.0\n",
    "Epoch: 1 | Loss: 1.00518, Accuracy: 89.54% | Test loss: 0.79164, AUC: 0.7681007298652949, MCC: 0.23497681155777314, sum: 5025.0\n",
    "Epoch: 2 | Loss: 0.67339, Accuracy: 78.65% | Test loss: 0.75463, AUC: 0.7986967979922242, MCC: 0.23954103427478463, sum: 12999.0\n",
    "Epoch: 3 | Loss: 0.61600, Accuracy: 86.49% | Test loss: 0.74335, AUC: 0.7968872367921364, MCC: 0.26001800489415483, sum: 7627.0\n",
    "Epoch: 4 | Loss: 0.47289, Accuracy: 85.79% | Test loss: 0.75486, AUC: 0.7990373875320378, MCC: 0.2609895855410456, sum: 8165.0\n",
    "Epoch: 5 | Loss: 0.43352, Accuracy: 80.06% | Test loss: 0.78084, AUC: 0.8002741989833781, MCC: 0.24479102586531848, sum: 12085.0\n",
    "Epoch: 6 | Loss: 0.43338, Accuracy: 85.32% | Test loss: 0.84007, AUC: 0.7894465876826939, MCC: 0.2561339791448311, sum: 8452.0\n",
    "Epoch: 7 | Loss: 0.28171, Accuracy: 84.58% | Test loss: 0.87809, AUC: 0.7905991596640114, MCC: 0.24990835735850425, sum: 8914.0\n",
    "Epoch: 8 | Loss: 0.20200, Accuracy: 87.29% | Test loss: 0.98652, AUC: 0.7812768510780681, MCC: 0.2568167667372486, sum: 6988.0\n",
    "Epoch: 9 | Loss: 0.23710, Accuracy: 88.13% | Test loss: 1.04308, AUC: 0.783673662642753, MCC: 0.2642501885152083, sum: 6431.0\n",
    "\n",
    "8M transfer learning:\n",
    "Epoch: 39 | Loss: 0.84134, Accuracy: 85.23% | Test loss: 0.72810, AUC: 0.7989206203533358, MCC: 0.25630946545449773, sum: 8860.0\n",
    "\n",
    "# NaN error:\n",
    "It started giving NaNs. I restarted a few times, rolled back, tried other things. Not sure what actually helped\n",
    "\n",
    "# 650M, batch size = 8:\n",
    "Epoch: 0 | Loss: 0.73659, Accuracy: 17.84% | Test loss: 1.04761, AUC: 0.4395135999006268, MCC: -0.053494933235651576, sum: 48365.0\n",
    "Epoch: 1 | Loss: 0.37189, Accuracy: 86.20% | Test loss: 0.60297, AUC: 0.8741129887987007, MCC: 0.3633713411500189, sum: 9149.0\n",
    "Epoch: 2 | Loss: 0.18138, Accuracy: 89.32% | Test loss: 0.61765, AUC: 0.8726657087875005, MCC: 0.3940537278837092, sum: 7042.0\n",
    "Epoch: 3 | Loss: 0.24919, Accuracy: 90.79% | Test loss: 0.79896, AUC: 0.8621024539783995, MCC: 0.3896582797271626, sum: 5727.0\n",
    "Epoch: 4 | Loss: 0.07897, Accuracy: 90.17% | Test loss: 0.81526, AUC: 0.8614779733187512, MCC: 0.38747794123876006, sum: 6235.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "def get_dataset_with_distances(annotation_path, tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances'):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    distances = []\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            protein_id = row[0].lower() + row[1]\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "\n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "\n",
    "            if len(distance) != len(sequence): \n",
    "                print(f'{protein_id} doesn\\'t match. Skipping ...')\n",
    "                break\n",
    "\n",
    "            # scale the distance\n",
    "            distance = scaler.transform(distance.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "            distances.append(distance)\n",
    "    train_tokenized = tokenizer(sequences) #, padding='max_length', truncation=True, max_length=MAX_LENGTH)# , max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    dataset = dataset.add_column(\"distances\", distances)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def train_scaler(annotation_path, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances'):\n",
    "    distances = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            protein_id = row[0].lower() + row[1]\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distances.append(distance)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate(distances).reshape(-1, 1))\n",
    "    return scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193, 833]) torch.Size([193, 833]) torch.Size([193, 833]) torch.Size([193, 833])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding=True,truncation=True)\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/src/fine-tuning/train.txt')\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/src/fine-tuning/train.txt', tokenizer, scaler)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/src/fine-tuning/val.txt', tokenizer, scaler)\n",
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForTokenClassificationESM(tokenizer) \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, collate_fn=data_collator) # shuffle=True, \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=data_collator)\n",
    "\n",
    "val_iterator = iter(val_dataloader)\n",
    "x = next(val_iterator)\n",
    "print(x['input_ids'].shape, x['attention_mask'].shape, x['labels'].shape, x['distances'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of EsmForTokenClassificationCustom were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight', 'distance_regressor.bias', 'distance_regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 2.62910, Accuracy: 22.17% | Test loss: 1.48222 - CBS: 1.028655767440796, distance: 0.4535630941390991, AUC: 0.45355163887608463, MCC: -0.04503063367232409, sum: 23.0\n",
      "Epoch: 1 | Loss: 2.75044, Accuracy: 93.80% | Test loss: 1.30920 - CBS: 0.901236891746521, distance: 0.4079667627811432, AUC: 0.6626167473062828, MCC: 0.1263499090393085, sum: 104.0\n",
      "Epoch: 2 | Loss: 2.84905, Accuracy: 90.45% | Test loss: 1.22923 - CBS: 0.8445392847061157, distance: 0.38468629121780396, AUC: 0.729914440872734, MCC: 0.2096951501950845, sum: 149.0\n",
      "Epoch: 3 | Loss: 2.70347, Accuracy: 88.89% | Test loss: 1.17809 - CBS: 0.7982466220855713, distance: 0.37984153628349304, AUC: 0.7637647883872939, MCC: 0.21954990130813484, sum: 156.0\n",
      "Epoch: 4 | Loss: 2.60240, Accuracy: 87.04% | Test loss: 1.15696 - CBS: 0.7784027457237244, distance: 0.3785569965839386, AUC: 0.7701274244147808, MCC: 0.22228374455587616, sum: 134.0\n",
      "Epoch: 5 | Loss: 2.72189, Accuracy: 87.56% | Test loss: 1.14809 - CBS: 0.7747660279273987, distance: 0.3733230531215668, AUC: 0.7686968511948825, MCC: 0.22066163051245696, sum: 130.0\n",
      "Epoch: 6 | Loss: 2.44252, Accuracy: 91.16% | Test loss: 1.16855 - CBS: 0.7875348329544067, distance: 0.38101813197135925, AUC: 0.7703092502825958, MCC: 0.24305482868032083, sum: 283.0\n",
      "Epoch: 7 | Loss: 2.17469, Accuracy: 85.97% | Test loss: 1.12884 - CBS: 0.7473706603050232, distance: 0.38147225975990295, AUC: 0.7871110170040901, MCC: 0.24728362070562615, sum: 311.0\n",
      "Epoch: 8 | Loss: 2.08891, Accuracy: 84.67% | Test loss: 1.16986 - CBS: 0.7638213634490967, distance: 0.4060414433479309, AUC: 0.7816456987472336, MCC: 0.23868473584388253, sum: 248.0\n",
      "Epoch: 9 | Loss: 1.95620, Accuracy: 88.51% | Test loss: 1.21713 - CBS: 0.8148676156997681, distance: 0.4022645652294159, AUC: 0.7787093023518064, MCC: 0.24603328812479305, sum: 419.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from torch import nn\n",
    "\n",
    "model = EsmForTokenClassificationCustom.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "distances_loss_fn = RMSLELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            output1, output2 = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            cbs_logits = output1.logits.flatten(1)\n",
    "            distance_logits = output2.logits.flatten(1)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  distances_loss_fn(valid_flattened_distance_logits, valid_flattened_distances)\n",
    "\n",
    "            test_loss = cbs_test_loss + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        output1, output2 = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cbs_logits = output1.logits.flatten(1)\n",
    "        distance_logits = output2.logits.flatten(1)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  distances_loss_fn(valid_flattened_distance_logits, valid_flattened_distances)\n",
    "\n",
    "        loss = cbs_loss + 4 * distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss}, distance: {distances_test_loss}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tryout: \n",
    "1. focal loss and a contrastive triplet center loss (https://academic.oup.com/bib/article/25/1/bbad488/7505238)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
