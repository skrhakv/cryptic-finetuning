{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.esm.modeling_esm import EsmPreTrainedModel, EsmModel\n",
    "import torch.nn as nn\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "LOSS = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "DROPOUT = 0.25\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "class EsmForTokenClassificationCustom(EsmPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "\n",
    "        # \n",
    "        # config.use_cache = False\n",
    "        super().__init__(config)\n",
    "        print(config)\n",
    "        self.esm = EsmModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(config.hidden_size, OUTPUT_SIZE)\n",
    "        self.distance_regressor = nn.Linear(config.hidden_size, OUTPUT_SIZE)\n",
    "        self.plDDT_regressor = nn.Linear(config.hidden_size, OUTPUT_SIZE)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.esm(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        # print(sequence_output.shape)\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        cbs_logits = self.classifier(sequence_output)\n",
    "        distance_logits = self.distance_regressor(sequence_output)\n",
    "        plddt_logits = self.plDDT_regressor(sequence_output)\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            logits=cbs_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )# , TokenClassifierOutput(\n",
    "        #     logits=distance_logits,\n",
    "        #     hidden_states=outputs.hidden_states,\n",
    "        #     attentions=outputs.attentions,\n",
    "        # ), TokenClassifierOutput(\n",
    "        #     logits=plddt_logits,\n",
    "        #     hidden_states=outputs.hidden_states,\n",
    "        #     attentions=outputs.attentions,\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 15:32:08.443140: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-02 15:32:08.481606: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-02 15:32:09.371167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.data.data_collator import DataCollatorMixin\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy\n",
    "# based on transformers DataCollatorForTokenClassification\n",
    "@dataclass\n",
    "class DataCollatorForTokenClassificationESM(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
    "            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def torch_call(self, features):\n",
    "        import torch\n",
    "        if \"distances\" in features[0].keys():\n",
    "            label_names = ['labels', 'distances']\n",
    "        else:\n",
    "            label_names = ['labels']\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = {label_name: [feature[label_name] for feature in features] for label_name in label_names}\n",
    "\n",
    "        no_labels_features = [{k: v for k, v in feature.items() if k not in label_names} for feature in features]\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            no_labels_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = batch[\"input_ids\"].shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "\n",
    "        def to_list(tensor_or_iterable):\n",
    "            if isinstance(tensor_or_iterable, torch.Tensor):\n",
    "                return tensor_or_iterable.tolist()\n",
    "            return list(tensor_or_iterable)\n",
    "\n",
    "        for label_name in label_names:\n",
    "            if padding_side == \"right\":\n",
    "                batch[label_name] = [\n",
    "                    # to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "                    # changed to pad the special tokens at the beginning and end of the sequence\n",
    "                    [self.label_pad_token_id] + to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label)-1) for label in labels[label_name]\n",
    "                ]\n",
    "            else:\n",
    "                batch[label_name] = [\n",
    "                    [self.label_pad_token_id] * (sequence_length - len(label)) + to_list(label) for label in labels[label_name]\n",
    "                ]\n",
    "    \n",
    "            batch[label_name] = torch.tensor(batch[label_name], dtype=torch.float)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of EsmForTokenClassificationCustom were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight', 'distance_regressor.bias', 'distance_regressor.weight', 'plDDT_regressor.bias', 'plDDT_regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmConfig {\n",
      "  \"_name_or_path\": \"facebook/esm2_t6_8M_UR50D\",\n",
      "  \"architectures\": [\n",
      "    \"EsmForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 320,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1280,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mask_token_id\": 32,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"esm\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 33\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# MODEL_NAME = 'facebook/esm2_t36_3B_UR50D'\n",
    "# MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "# MODEL_NAME = 'facebook/esm2_t30_150M_UR50D'\n",
    "MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
    "\n",
    "# TODO: try torch_dtype=torch.bfloat16\n",
    "model = EsmForTokenClassificationCustom.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "def get_dataset(annotation_path, tokenizer):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "\n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "\n",
    "            assert len(sequence) == len(label)\n",
    "\n",
    "    train_tokenized = tokenizer(sequences, max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer)\n",
    "val_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassificationESM(tokenizer) \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, shuffle=True, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.27190, Accuracy: 92.94% | Test loss: 1.34428, AUC: 0.7471983781967955, MCC: 0.2704698714087362, sum: 2488.0\n",
      "Epoch: 1 | Loss: 0.27539, Accuracy: 89.50% | Test loss: 1.23362, AUC: 0.7643900761301369, MCC: 0.2460163020180396, sum: 5081.0\n",
      "Epoch: 2 | Loss: 0.19173, Accuracy: 88.98% | Test loss: 1.31812, AUC: 0.7681208232947736, MCC: 0.2472843175379563, sum: 5488.0\n",
      "Epoch: 3 | Loss: 0.10782, Accuracy: 91.13% | Test loss: 1.52325, AUC: 0.7705293194415301, MCC: 0.2629112046245141, sum: 3965.0\n",
      "Epoch: 4 | Loss: 0.08166, Accuracy: 91.15% | Test loss: 1.69592, AUC: 0.7622030622471703, MCC: 0.2549547849067751, sum: 3871.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def print_used_memory():\n",
    "    free, total = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
    "    mem_used_MB = (total - free) / 1024 ** 2\n",
    "    mem_total_MB = (total) / 1024 ** 2\n",
    "    print(f'{mem_used_MB} MB / {mem_total_MB} MB')\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "# with torch.autocast(device_type='cuda'):\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "        \n",
    "            logits = output.logits.flatten(1)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100].float()\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                    y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            del input_ids, attention_mask, labels, logits, valid_flattened_logits, valid_flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        # print('before train prediction')\n",
    "        # print_used_memory()\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits.flatten(1)\n",
    "        labels = batch['labels'].to(device)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print('after train prediction')\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 35M:\n",
    "# Epoch: 3 | Loss: 0.26539, Accuracy: 87.91% | Test loss: 0.71823, AUC: 0.8429591457364806, MCC: 0.33372585153627116, sum: 7430.0\n",
    "\n",
    "# 150M:\n",
    "# Epoch: 2 | Loss: 0.47842, Accuracy: 89.44% | Test loss: 0.63896, AUC: 0.8584368023837566, MCC: 0.36990818473863774, sum: 6639.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def print_used_memory():\n",
    "    free, total = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
    "    mem_used_MB = (total - free) / 1024 ** 2\n",
    "    mem_total_MB = (total) / 1024 ** 2\n",
    "    print(f'{mem_used_MB} MB / {mem_total_MB} MB')\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    print('Before test:')\n",
    "    print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            print('before prediction:')\n",
    "\n",
    "            print_used_memory()\n",
    "\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            print('after prediction:')\n",
    "            print_used_memory()\n",
    "        \n",
    "            logits = output.logits.flatten(1)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss =  loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            del input_ids, attention_mask, labels, logits, valid_flattened_logits, valid_flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print('after test')\n",
    "    print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        print('before train prediction')\n",
    "        print_used_memory()\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits.flatten(1)\n",
    "        labels = batch['labels'].to(device)\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 35M:\n",
    "# Epoch: 3 | Loss: 0.26539, Accuracy: 87.91% | Test loss: 0.71823, AUC: 0.8429591457364806, MCC: 0.33372585153627116, sum: 7430.0\n",
    "\n",
    "# 150M:\n",
    "# Epoch: 2 | Loss: 0.47842, Accuracy: 89.44% | Test loss: 0.63896, AUC: 0.8584368023837566, MCC: 0.36990818473863774, sum: 6639.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before test:\n",
    "3043.0 MB / 81153.75 MB\n",
    "before prediction:\n",
    "3043.0 MB / 81153.75 MB\n",
    "after prediction:\n",
    "31835.0 MB / 81153.75 MB\n",
    "after test\n",
    "31837.0 MB / 81153.75 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8M:\n",
    "\n",
    "Epoch: 0 | Loss: 0.78285, Accuracy: 18.43% | Test loss: 1.03734, AUC: 0.42720166912931357, MCC: -0.054964089541398004, sum: 47960.0\n",
    "Epoch: 1 | Loss: 1.00518, Accuracy: 89.54% | Test loss: 0.79164, AUC: 0.7681007298652949, MCC: 0.23497681155777314, sum: 5025.0\n",
    "Epoch: 2 | Loss: 0.67339, Accuracy: 78.65% | Test loss: 0.75463, AUC: 0.7986967979922242, MCC: 0.23954103427478463, sum: 12999.0\n",
    "Epoch: 3 | Loss: 0.61600, Accuracy: 86.49% | Test loss: 0.74335, AUC: 0.7968872367921364, MCC: 0.26001800489415483, sum: 7627.0\n",
    "Epoch: 4 | Loss: 0.47289, Accuracy: 85.79% | Test loss: 0.75486, AUC: 0.7990373875320378, MCC: 0.2609895855410456, sum: 8165.0\n",
    "Epoch: 5 | Loss: 0.43352, Accuracy: 80.06% | Test loss: 0.78084, AUC: 0.8002741989833781, MCC: 0.24479102586531848, sum: 12085.0\n",
    "Epoch: 6 | Loss: 0.43338, Accuracy: 85.32% | Test loss: 0.84007, AUC: 0.7894465876826939, MCC: 0.2561339791448311, sum: 8452.0\n",
    "Epoch: 7 | Loss: 0.28171, Accuracy: 84.58% | Test loss: 0.87809, AUC: 0.7905991596640114, MCC: 0.24990835735850425, sum: 8914.0\n",
    "Epoch: 8 | Loss: 0.20200, Accuracy: 87.29% | Test loss: 0.98652, AUC: 0.7812768510780681, MCC: 0.2568167667372486, sum: 6988.0\n",
    "Epoch: 9 | Loss: 0.23710, Accuracy: 88.13% | Test loss: 1.04308, AUC: 0.783673662642753, MCC: 0.2642501885152083, sum: 6431.0\n",
    "\n",
    "8M transfer learning:\n",
    "Epoch: 39 | Loss: 0.84134, Accuracy: 85.23% | Test loss: 0.72810, AUC: 0.7989206203533358, MCC: 0.25630946545449773, sum: 8860.0\n",
    "\n",
    "# NaN error:\n",
    "It started giving NaNs. I restarted a few times, rolled back, tried other things. Not sure what actually helped\n",
    "\n",
    "# 650M, batch size = 8:\n",
    "Epoch: 0 | Loss: 0.73659, Accuracy: 17.84% | Test loss: 1.04761, AUC: 0.4395135999006268, MCC: -0.053494933235651576, sum: 48365.0\n",
    "Epoch: 1 | Loss: 0.37189, Accuracy: 86.20% | Test loss: 0.60297, AUC: 0.8741129887987007, MCC: 0.3633713411500189, sum: 9149.0\n",
    "Epoch: 2 | Loss: 0.18138, Accuracy: 89.32% | Test loss: 0.61765, AUC: 0.8726657087875005, MCC: 0.3940537278837092, sum: 7042.0\n",
    "Epoch: 3 | Loss: 0.24919, Accuracy: 90.79% | Test loss: 0.79896, AUC: 0.8621024539783995, MCC: 0.3896582797271626, sum: 5727.0\n",
    "Epoch: 4 | Loss: 0.07897, Accuracy: 90.17% | Test loss: 0.81526, AUC: 0.8614779733187512, MCC: 0.38747794123876006, sum: 6235.0\n",
    "\n",
    "(to compare with 650M transfer learning:)\n",
    "(Epoch: 19 | Loss: 0.57333, Accuracy: 87.17% | Test loss: 0.61883, AUC: 0.8642494368067841, MCC: 0.3501214427034594, sum: 8540.0)\n",
    "so 0.01 AUC improvement, not great not terrible\n",
    "\n",
    "# 650M MULTITASK learning:\n",
    "Epoch: 0 | Loss: 0.64907, Accuracy: 27.87% | Test loss: 1.51112 - CBS: 1.3719383478164673, distance: 0.13918635249137878, AUC: 0.43274620440958667, MCC: -0.0542650498061712, sum: 111.0\n",
    "Epoch: 1 | Loss: 0.48987, Accuracy: 83.13% | Test loss: 0.87216 - CBS: 0.8713946342468262, distance: 0.0007648273603990674, AUC: 0.8760669807589696, MCC: 0.3457642510325305, sum: 80.0\n",
    "Epoch: 2 | Loss: 0.10602, Accuracy: 83.81% | Test loss: 0.99556 - CBS: 0.9947194457054138, distance: 0.0008437958895228803, AUC: 0.8668396447010817, MCC: 0.33656286186003864, sum: 36.0\n",
    "Epoch: 3 | Loss: 0.45065, Accuracy: 85.19% | Test loss: 1.37749 - CBS: 1.376583456993103, distance: 0.0009044440812431276, AUC: 0.855343432123152, MCC: 0.3286448894311547, sum: 88.0\n",
    "Epoch: 4 | Loss: 0.09049, Accuracy: 86.44% | Test loss: 1.36612 - CBS: 1.3654472827911377, distance: 0.0006769609753973782, AUC: 0.8566140648513804, MCC: 0.3439374629449201, sum: 26.0\n",
    "Epoch: 5 | Loss: 0.11011, Accuracy: 88.46% | Test loss: 1.60821 - CBS: 1.6076503992080688, distance: 0.0005574710085056722, AUC: 0.8539008246932507, MCC: 0.3623427913775662, sum: 22.0\n",
    "Epoch: 6 | Loss: 0.12407, Accuracy: 88.35% | Test loss: 1.73687 - CBS: 1.73642098903656, distance: 0.000453823187854141, AUC: 0.8575289308249205, MCC: 0.3587223828788271, sum: 73.0\n",
    "Epoch: 7 | Loss: 0.00377, Accuracy: 90.80% | Test loss: 2.10933 - CBS: 2.1088972091674805, distance: 0.00043249232112430036, AUC: 0.8599045856585643, MCC: 0.3764254979736856, sum: 19.0\n",
    "Epoch: 8 | Loss: 0.09617, Accuracy: 90.60% | Test loss: 2.00999 - CBS: 2.009589195251465, distance: 0.00039868077146820724, AUC: 0.8536332037559458, MCC: 0.37197599850317287, sum: 53.0\n",
    "Epoch: 9 | Loss: 0.03469, Accuracy: 89.00% | Test loss: 1.89704 - CBS: 1.8966468572616577, distance: 0.0003907561185769737, AUC: 0.8506434369981348, MCC: 0.3449904319459605, sum: 24.0\n",
    "# 650M boosted cbs-loss:\n",
    "Epoch: 0 | Loss: 2.98668, Accuracy: 50.06% | Test loss: 1.27300 - CBS: 1.0052306652069092, distance: 0.2677696645259857, AUC: 0.4612774687124019, MCC: -0.023694198792789625, sum: 27406.0\n",
    "Epoch: 1 | Loss: 1.02846, Accuracy: 89.12% | Test loss: 0.58897 - CBS: 0.5878161787986755, distance: 0.0011529671028256416, AUC: 0.8777632042771085, MCC: 0.4059588496168788, sum: 7218.0\n",
    "Epoch: 2 | Loss: 0.55701, Accuracy: 87.81% | Test loss: 0.64764 - CBS: 0.6457691788673401, distance: 0.0018734950572252274, AUC: 0.8685130445474384, MCC: 0.3790404140190229, sum: 7928.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "def get_dataset_with_distances(annotation_path, tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    distances = []\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "            \n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distance = np.clip(distance, 0, 10)\n",
    "\n",
    "            if len(distance) != len(sequence): \n",
    "                print(f'{protein_id} doesn\\'t match. Skipping ...')\n",
    "                break\n",
    "\n",
    "            # scale the distance\n",
    "            distance = scaler.transform(distance.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "            distances.append(distance)\n",
    "    train_tokenized = tokenizer(sequences) #, padding='max_length', truncation=True, max_length=MAX_LENGTH)# , max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    dataset = dataset.add_column(\"distances\", distances)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def train_scaler(annotation_path, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "    distances = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distances.append(distance)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.concatenate(distances).reshape(-1, 1))\n",
    "    return scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding=True,truncation=True)\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/src/fine-tuning/train.txt')\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/src/fine-tuning/train.txt', tokenizer, scaler)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/src/fine-tuning/val.txt', tokenizer, scaler)\n",
    "\n",
    "# scaler = train_scaler('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt')\n",
    "# train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer, scaler)\n",
    "# val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForTokenClassificationESM(tokenizer) \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=data_collator)\n",
    "\n",
    "# val_iterator = iter(val_dataloader)\n",
    "# x = next(val_iterator)\n",
    "# print(x['input_ids'].shape, x['attention_mask'].shape, x['labels'].shape, x['distances'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(pred + 1, actual + 1))\n",
    "\n",
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred + 1), torch.log(actual + 1))\n",
    "\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of EsmForTokenClassificationCustom were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight', 'distance_regressor.bias', 'distance_regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 2.98668, Accuracy: 50.06% | Test loss: 1.27300 - CBS: 1.0052306652069092, distance: 0.2677696645259857, AUC: 0.4612774687124019, MCC: -0.023694198792789625, sum: 27406.0\n",
      "Epoch: 1 | Loss: 1.02846, Accuracy: 89.12% | Test loss: 0.58897 - CBS: 0.5878161787986755, distance: 0.0011529671028256416, AUC: 0.8777632042771085, MCC: 0.4059588496168788, sum: 7218.0\n",
      "Epoch: 2 | Loss: 0.55701, Accuracy: 87.81% | Test loss: 0.64764 - CBS: 0.6457691788673401, distance: 0.0018734950572252274, AUC: 0.8685130445474384, MCC: 0.3790404140190229, sum: 7928.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 110\u001b[0m\n\u001b[1;32m    106\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    108\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 110\u001b[0m     batch_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    112\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    114\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(batch_losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_losses))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from torch import nn\n",
    "\n",
    "model = EsmForTokenClassificationCustom.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "distances_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            output1, output2 = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            cbs_logits = output1.logits.flatten(1)\n",
    "            distance_logits = output2.logits.flatten(1)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  distances_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "\n",
    "            test_loss = cbs_test_loss + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        output1, output2 = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cbs_logits = output1.logits.flatten(1)\n",
    "        distance_logits = output2.logits.flatten(1)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  distances_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "# different loss, sigmoid\n",
    "        loss = 4 * cbs_loss +  distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss}, distance: {distances_test_loss}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tryout: \n",
    "1. focal loss and a contrastive triplet center loss (https://academic.oup.com/bib/article/25/1/bbad488/7505238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding=True,truncation=True)\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForTokenClassificationESM(tokenizer) \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=data_collator) \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=data_collator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of EsmForTokenClassificationCustom were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight', 'distance_regressor.bias', 'distance_regressor.weight', 'plDDT_regressor.bias', 'plDDT_regressor.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.83850, Accuracy: 7.99% | Test loss: 1.10899 - CBS: 1.1089880466461182, AUC: 0.40072266332934214, MCC: -0.06774546446956191, sum: 53778.0\n",
      "Epoch: 1 | Loss: 0.37627, Accuracy: 84.18% | Test loss: 0.65449 - CBS: 0.654488742351532, AUC: 0.8773803128224543, MCC: 0.35904512468287153, sum: 10427.0\n",
      "Epoch: 2 | Loss: 0.22312, Accuracy: 80.66% | Test loss: 0.67400 - CBS: 0.6740042567253113, AUC: 0.8757899214885686, MCC: 0.33932737993656564, sum: 12702.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from torch import nn\n",
    "\n",
    "model = EsmForTokenClassificationCustom.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 3\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            output1, _, output3 = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            cbs_logits = output1.logits.flatten(1)\n",
    "            distance_logits = output3.logits.flatten(1)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            # distances_test_loss =  distances_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "\n",
    "            test_loss = cbs_test_loss # + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        output1, _, output3 = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cbs_logits = output1.logits.flatten(1)\n",
    "        distance_logits = output3.logits.flatten(1)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "        \n",
    "        # different loss, sigmoid\n",
    "        loss = cbs_loss +  distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding=True,truncation=True)\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt')\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer, scaler)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "data_collator = DataCollatorForTokenClassificationESM(tokenizer) \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.51899, Accuracy: 86.57% | Test loss: 0.94554 - CBS: 0.7093595862388611, distance: 0.23618388175964355, AUC: 0.8648569544953557, MCC: 0.3733514022556253, sum: 8817.0\n",
      "Epoch: 1 | Loss: 0.47167, Accuracy: 89.55% | Test loss: 0.80418 - CBS: 0.5722964406013489, distance: 0.23188810050487518, AUC: 0.8841012865297871, MCC: 0.4222520065360723, sum: 7076.0\n",
      "Epoch: 2 | Loss: 0.05917, Accuracy: 88.98% | Test loss: 0.93403 - CBS: 0.6853230595588684, distance: 0.24871017038822174, AUC: 0.8669081084432939, MCC: 0.39911661201568155, sum: 7247.0\n",
      "Epoch: 3 | Loss: 0.31031, Accuracy: 91.95% | Test loss: 1.11387 - CBS: 0.8524396419525146, distance: 0.2614312171936035, AUC: 0.8680197789784566, MCC: 0.42306381062307064, sum: 5002.0\n",
      "Epoch: 4 | Loss: 0.04782, Accuracy: 92.16% | Test loss: 1.25636 - CBS: 0.9804365038871765, distance: 0.2759261131286621, AUC: 0.8705917051782379, MCC: 0.43595482054907025, sum: 4965.0\n",
      "Epoch: 5 | Loss: 0.09336, Accuracy: 91.67% | Test loss: 1.37343 - CBS: 1.0879831314086914, distance: 0.2854474186897278, AUC: 0.870417400530627, MCC: 0.42737542173245385, sum: 5304.0\n",
      "Epoch: 6 | Loss: 0.02300, Accuracy: 91.73% | Test loss: 1.31431 - CBS: 1.043104648590088, distance: 0.2712089717388153, AUC: 0.8652156741428216, MCC: 0.4138795860687163, sum: 5089.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m loss \u001b[38;5;241m=\u001b[39m cbs_loss \u001b[38;5;66;03m# +  distances_loss\u001b[39;00m\n\u001b[1;32m     84\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 86\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     90\u001b[0m batch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/deeplife/venv/lib64/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deeplife/venv/lib64/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "distances_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            output1, output2, _ = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            cbs_logits = output1.logits.flatten(1)\n",
    "            distance_logits = output2.logits.flatten(1)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  distances_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "\n",
    "            test_loss = cbs_test_loss + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        output1, output2, _ = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cbs_logits = output1.logits.flatten(1)\n",
    "        distance_logits = output2.logits.flatten(1)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        # distances_loss =  distances_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "# different loss, sigmoid\n",
    "        loss = cbs_loss # +  distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss}, distance: {distances_test_loss}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
