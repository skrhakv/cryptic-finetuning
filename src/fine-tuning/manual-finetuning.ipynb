{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b2fd7b8f554a9198a612de67d0208e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EsmModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import csv\n",
    "from datasets import Dataset\n",
    "import functools\n",
    "\n",
    "DROPOUT = 0.25\n",
    "OUTPUT_SIZE = 1\n",
    "MAX_LENGTH = 1024\n",
    "LABEL_PAD_TOKEN_ID = -100\n",
    "\n",
    "# MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
    "# MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "MODEL_NAME = 'facebook/esm2_t36_3B_UR50D'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def print_used_memory():\n",
    "    free, total = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
    "    mem_used_MB = (total - free) / 1024 ** 2\n",
    "    mem_total_MB = (total) / 1024 ** 2\n",
    "    print(f'{mem_used_MB} MB / {mem_total_MB} MB')\n",
    "\n",
    "class FinetuneESM(nn.Module):\n",
    "    def __init__(self, esm_model: str) -> None:\n",
    "        super().__init__()\n",
    "        self.llm = EsmModel.from_pretrained(esm_model) #, torch_dtype=torch.bfloat16)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE) # , dtype=torch.bfloat16)\n",
    "        \n",
    "    def forward(self, batch: dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        token_embeddings = self.llm(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "        return self.classifier(token_embeddings)\n",
    "\n",
    "def get_dataset(annotation_path, tokenizer):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "\n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence)) #, dtype=np.float16)\n",
    "            label[indices] = 1\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "\n",
    "            assert len(sequence) == len(label)\n",
    "\n",
    "    tokenized_sequences = tokenizer(sequences, max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "\n",
    "    dataset = Dataset.from_dict(tokenized_sequences)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "\n",
    "    if \"distances\" in batch[0].keys():\n",
    "        label_names = ['labels', 'distances']\n",
    "    else:\n",
    "        label_names = ['labels']\n",
    "\n",
    "    labels = {label_name: [feature[label_name] for feature in batch] for label_name in label_names}\n",
    "    no_labels_features = [{k: v for k, v in feature.items() if k not in label_names} for feature in batch]\n",
    "\n",
    "    batch = tokenizer.pad(\n",
    "        no_labels_features,\n",
    "        padding=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    # type to bfloat16\n",
    "    # batch['input_ids'] = batch['input_ids'].to(dtype=torch.float16).to(DEVICE)\n",
    "    # batch['attention_mask'] = batch['attention_mask'].to(dtype=torch.float16).to(DEVICE)\n",
    "\n",
    "    sequence_length = batch[\"input_ids\"].shape[1]\n",
    "\n",
    "    for label_name in label_names:\n",
    "        batch[label_name] = [[LABEL_PAD_TOKEN_ID] + list(label) + [LABEL_PAD_TOKEN_ID] * (sequence_length - len(label)-1) for label in labels[label_name]]\n",
    "        batch[label_name] = torch.tensor(batch[label_name]).to(DEVICE)# , dtype=torch.bfloat16).to(DEVICE)\n",
    "\n",
    "    return batch\n",
    "\n",
    "model = FinetuneESM(MODEL_NAME).half().to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "partial_collate_fn = functools.partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "# train_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer)\n",
    "# val_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "# \n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=partial_collate_fn)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train end-to-end without lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6097.0625 MB / 81090.125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.93886, Accuracy: 75.37% | Test loss: 0.98541, AUC: 0.4966444777008342, MCC: 0.00851241159477756, sum: 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.41030, Accuracy: 86.53% | Test loss: 0.58455, AUC: 0.8831857513908667, MCC: 0.38747096053283003, sum: 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.16598, Accuracy: 89.82% | Test loss: 0.56725, AUC: 0.8894166977418276, MCC: 0.4210427297022199, sum: 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 1.01018, Accuracy: 91.55% | Test loss: 0.68505, AUC: 0.8826773671146894, MCC: 0.424533219608909, sum: 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.93870, Accuracy: 94.32% | Test loss: 0.94482, AUC: 0.5987813101629393, MCC: 0.0, sum: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1524b8ac7950>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[32m     91\u001b[39m optimizer.zero_grad()\n\u001b[32m     93\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m batch_losses.append(loss.cpu().float().detach().numpy())\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py:292\u001b[39m, in \u001b[36mOptimizer8bit.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.prefetch_state(p)\n\u001b[32m    291\u001b[39m         \u001b[38;5;28mself\u001b[39m.update_step(group, p, gindex, pindex)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_paged:\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# all paged operation are asynchronous, we need\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;66;03m# to sync to make sure all tensors are in the right state\u001b[39;00m\n\u001b[32m    296\u001b[39m     torch.cuda.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/cuda/__init__.py:985\u001b[39m, in \u001b[36msynchronize\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    983\u001b[39m _lazy_init()\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.device(device):\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import gc\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=0.0001, eps=1e-4) \n",
    "# optimizer = torch.optim.AdamW8bit(params=model.parameters(),\n",
    "#                             lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # input_ids = batch['input_ids'].to(DEVICE)\n",
    "            # attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(batch)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "            logits = output.flatten(1)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]# .float()\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(valid_flattened_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        output = model(batch) #, attention_mask=attention_mask)\n",
    "        logits = output.flatten(1)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 850M without LORA - here:\n",
    "# Epoch: 1 | Loss: 0.26357, Accuracy: 88.84% | Test loss: 0.59217, AUC: 0.8772442293540929, MCC: 0.3847349478773821, sum: 7178.0\n",
    "# \n",
    "# 850M without LORA - custom-finetuning.ipynb:\n",
    "# Epoch: 1 | Loss: 0.34253, Accuracy: 91.27% | Test loss: 0.59555, AUC: 0.878478909678348, MCC: 0.41902639123765595, sum: 5562.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Any other classifier head needs to be added here! (in future: apo-holo distance/plDDT regresor)\n",
      "3081.0625 MB / 81090.125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.07641, Accuracy: 84.94% | Test loss: 0.96781, AUC: 0.5584380776297676, MCC: 0.06338426312302034, sum: 6467.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.98292, Accuracy: 90.08% | Test loss: 0.70329, AUC: 0.828017489151323, MCC: 0.33694228067099186, sum: 5599.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.28898, Accuracy: 94.32% | Test loss: 0.92707, AUC: 0.5952360896233821, MCC: 0.0, sum: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.93123, Accuracy: 86.97% | Test loss: 0.63380, AUC: 0.8560885121193611, MCC: 0.3594092927832279, sum: 8330.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.35971, Accuracy: 86.21% | Test loss: 0.64358, AUC: 0.8543168563973295, MCC: 0.3511116215049817, sum: 8806.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 0.20516, Accuracy: 89.37% | Test loss: 0.70072, AUC: 0.8532096263216228, MCC: 0.3716428972575048, sum: 6588.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 101\u001b[0m\n\u001b[1;32m     97\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 101\u001b[0m batch_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n\u001b[1;32m    104\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/skrhakv/cryptic-nn/src/fine-tuning')\n",
    "\n",
    "from lora import apply_lora # , freeze_all_but_head\n",
    "\n",
    "apply_lora(model) # .to(DEVICE)\n",
    "\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # input_ids = batch['input_ids'].to(DEVICE)\n",
    "            # attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(batch)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "            logits = output.flatten(1)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]# .float()\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(valid_flattened_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        output = model(batch) #, attention_mask=attention_mask)\n",
    "        logits = output.flatten(1)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 850M without LORA - here:\n",
    "# Epoch: 1 | Loss: 0.26357, Accuracy: 88.84% | Test loss: 0.59217, AUC: 0.8772442293540929, MCC: 0.3847349478773821, sum: 7178.0\n",
    "# \n",
    "# 850M with LORA - here:\n",
    "# Epoch: 3 | Loss: 0.93123, Accuracy: 86.97% | Test loss: 0.63380, AUC: 0.8560885121193611, MCC: 0.3594092927832279, sum: 8330.0\n",
    "#\n",
    "# 850M without LORA - custom-finetuning.ipynb:\n",
    "# Epoch: 1 | Loss: 0.34253, Accuracy: 91.27% | Test loss: 0.59555, AUC: 0.878478909678348, MCC: 0.41902639123765595, sum: 5562.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForTokenClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained('Synthyra/ESMplusplus_large', num_labels=1, problem_type='single_label_classification').to(DEVICE)\n",
    "tokenizer = model.tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esm++ tryout .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18529.0625 MB / 81090.125 MB\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([0.1934], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.80839, Accuracy: 19.61% | Test loss: 1.76119, AUC: 0.4258982469501052, MCC: -0.07413211318761029, sum: 46029.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([0.0760], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.38677, Accuracy: 82.28% | Test loss: 0.62088, AUC: 0.8778704881941413, MCC: 0.3450996010979654, sum: 11621.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-0.1264], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.09270, Accuracy: 88.66% | Test loss: 0.66985, AUC: 0.8738272595540745, MCC: 0.3814881770277971, sum: 7282.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-0.7474], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.19809, Accuracy: 92.79% | Test loss: 0.98229, AUC: 0.8650654748465431, MCC: 0.4265198775537588, sum: 4246.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-1.1127], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.03363, Accuracy: 90.89% | Test loss: 1.24782, AUC: 0.8564089380902804, MCC: 0.38666894885946734, sum: 5505.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-0.6319], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 0.04672, Accuracy: 91.43% | Test loss: 1.16187, AUC: 0.8604517740366541, MCC: 0.4091554389917905, sum: 5298.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-0.9140], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss: 0.05581, Accuracy: 92.45% | Test loss: 1.34929, AUC: 0.8511572368989107, MCC: 0.39652730919182694, sum: 4219.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-1.0599], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss: 0.00296, Accuracy: 92.82% | Test loss: 1.95489, AUC: 0.8424755979534786, MCC: 0.3872959655490615, sum: 3764.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-1.2158], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss: 0.06317, Accuracy: 92.07% | Test loss: 1.72772, AUC: 0.8482709805426195, MCC: 0.3931933804662767, sum: 4532.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-1.6969], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss: 0.00201, Accuracy: 92.85% | Test loss: 2.02877, AUC: 0.8289615701010321, MCC: 0.36783836976154755, sum: 3526.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "partial_collate_fn = functools.partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer)\n",
    "val_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "            # print(output)\n",
    "            logits = output.logits.flatten(1)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            # print(output.logits.shape, flattened_labels.shape)\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]# .float()\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(valid_flattened_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask) #, attention_mask=attention_mask)\n",
    "        logits = output.logits.flatten(1)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 850M without LORA - here:\n",
    "# Epoch: 1 | Loss: 0.26357, Accuracy: 88.84% | Test loss: 0.59217, AUC: 0.8772442293540929, MCC: 0.3847349478773821, sum: 7178.0\n",
    "# \n",
    "# 850M with LORA - here:\n",
    "# Epoch: 3 | Loss: 0.93123, Accuracy: 86.97% | Test loss: 0.63380, AUC: 0.8560885121193611, MCC: 0.3594092927832279, sum: 8330.0\n",
    "#\n",
    "# 850M without LORA - custom-finetuning.ipynb:\n",
    "# Epoch: 1 | Loss: 0.34253, Accuracy: 91.27% | Test loss: 0.59555, AUC: 0.878478909678348, MCC: 0.41902639123765595, sum: 5562.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "def get_dataset_with_distances(annotation_path, tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    distances = []\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "            \n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distance = np.clip(distance, 0, 10)\n",
    "\n",
    "            if len(distance) != len(sequence): \n",
    "                print(f'{protein_id} doesn\\'t match. Skipping ...')\n",
    "                break\n",
    "\n",
    "            # scale the distance\n",
    "            distance = scaler.transform(distance.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "            distances.append(distance)\n",
    "    train_tokenized = tokenizer(sequences) #, padding='max_length', truncation=True, max_length=MAX_LENGTH)# , max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    dataset = dataset.add_column(\"distances\", distances)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def train_scaler(annotation_path, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "    distances = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distances.append(distance)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.concatenate(distances).reshape(-1, 1))\n",
    "    return scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForTokenClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.89152, Accuracy: 54.79% | Test loss: 1.08529 - CBS: 1.08529, AUC: 0.6218540, MCC: 0.08392, sum: 25996.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.53873, Accuracy: 83.67% | Test loss: 0.71629 - CBS: 0.71629, AUC: 0.8344590, MCC: 0.30053, sum: 10030.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.63747, Accuracy: 75.40% | Test loss: 0.76744 - CBS: 0.76744, AUC: 0.8411284, MCC: 0.27558, sum: 15445.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    120\u001b[39m loss = cbs_loss +  distances_loss\n\u001b[32m    121\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m optimizer.step()\n\u001b[32m    127\u001b[39m batch_losses.append(loss.cpu().detach().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained('Synthyra/ESMplusplus_large', num_labels=3).to(DEVICE)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# train only the classifier for the first few epochs \n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('transformer'): # choose whatever you like here\n",
    "        param.requires_grad = False\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn) \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 4\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # unfreeze and do end-to-end finetuning after training only the classifier for the first few epochs \n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            cbs_logits = output.logits[:,:,0]\n",
    "            distance_logits = output.logits[:,:,1]\n",
    "            \n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            # distances_test_loss =  distances_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "\n",
    "            test_loss = cbs_test_loss # + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cbs_logits = output.logits[:,:,0]\n",
    "        distance_logits = output.logits[:,:,1]\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "        \n",
    "        # different loss, sigmoid\n",
    "        loss = cbs_loss +  distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss:.5f}, AUC: {roc_auc:.7f}, MCC: {mcc:.5f}, sum: {sum(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.45650, Accuracy: 90.52% | Test loss: 0.81983 - CBS: 0.81983, AUC: 0.8717390, MCC: 0.42576, sum: 6302.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.11850, Accuracy: 89.87% | Test loss: 0.94013 - CBS: 0.94013, AUC: 0.8638283, MCC: 0.40362, sum: 6570.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    114\u001b[39m loss = cbs_loss +  distances_loss\n\u001b[32m    115\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m optimizer.step()\n\u001b[32m    121\u001b[39m batch_losses.append(loss.cpu().detach().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt')\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer, scaler)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# similarly train only the classifier for the first few epochs \n",
    "# for name, param in model.named_parameters():\n",
    "#      if name.startswith('transformer'): # choose whatever you like here\n",
    "#         param.requires_grad = False\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EPOCHS = 8\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # unfreeze and do end-to-end finetuning after training only the classifier for the first few epochs \n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            cbs_logits = output.logits[:,:,0]\n",
    "            distance_logits = output.logits[:,:,2]\n",
    "            \n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            # distances_test_loss =  distances_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "\n",
    "            test_loss = cbs_test_loss # + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cbs_logits = output.logits[:,:,0]\n",
    "        distance_logits = output.logits[:,:,2]\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "        \n",
    "        # different loss, sigmoid\n",
    "        loss = cbs_loss +  distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss:.5f}, AUC: {roc_auc:.7f}, MCC: {mcc:.5f}, sum: {sum(predictions)}\")\n",
    "\n",
    "# ligysis - freezing first 3 epochs, then 3 epochs full; CryptoBench - freezing first 3 epochs, then full:\n",
    "# Epoch: 3 | Loss: 0.51589, Accuracy: 89.64% | Test loss: 0.57979 - CBS: 0.57979, AUC: 0.8856013, MCC: 0.42233, sum: 7002.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emsC tryout ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "\n",
    "def get_dataset(annotation_path, tokenizer):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "\n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence)) #, dtype=np.float16)\n",
    "            label[indices] = 1\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "\n",
    "            assert len(sequence) == len(label)\n",
    "\n",
    "    tokenized_sequences = tokenizer(sequences, max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "\n",
    "    dataset = Dataset.from_dict(tokenized_sequences)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "ESMC_DIM = 1152\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "class EsmForTokenClassificationCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.esm = ESMC.from_pretrained(\"esmc_600m\")\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(ESMC_DIM, OUTPUT_SIZE)\n",
    "        self.tokenizer = self.esm.tokenizer\n",
    "\n",
    "    def forward(self, input, sequence_id):\n",
    "        outputs = self.esm(input, sequence_id).embeddings.float()\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.classifier(outputs)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForTokenClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0f6707e1ff4a2a9c2cc8a384fb0607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('Synthyra/ESMplusplus_large', num_labels=3).to(DEVICE)\n",
    "tokenizer = model.tokenizer\n",
    "model = EsmForTokenClassificationCustom()\n",
    "\n",
    "\n",
    "train_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', model.tokenizer)\n",
    "val_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', model.tokenizer)\n",
    "\n",
    "partial_collate_fn = functools.partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15981.0625 MB / 81090.125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.11461, Accuracy: 81.13% | Test loss: 0.99226, AUC: 0.5952734257314609, MCC: 0.05722997355599492, sum: 8874.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.98699, Accuracy: 93.82% | Test loss: 0.96832, AUC: 0.7470172014095504, MCC: 0.17029190371498892, sum: 955.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.74901, Accuracy: 93.79% | Test loss: 0.94789, AUC: 0.7685537711124515, MCC: 0.19150300902632247, sum: 1111.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.96995, Accuracy: 93.58% | Test loss: 0.92987, AUC: 0.777330043196548, MCC: 0.20147752062808624, sum: 1366.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.38523, Accuracy: 88.93% | Test loss: 0.60264, AUC: 0.8757627772921935, MCC: 0.41047776514573, sum: 7433.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 0.16317, Accuracy: 89.85% | Test loss: 0.59487, AUC: 0.8734679599282306, MCC: 0.4145183582869328, sum: 6728.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss: 0.36034, Accuracy: 91.64% | Test loss: 0.65518, AUC: 0.8691317395028082, MCC: 0.42025814196812533, sum: 5246.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss: 0.14920, Accuracy: 92.15% | Test loss: 0.70656, AUC: 0.86906248948034, MCC: 0.4257054828357001, sum: 4843.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss: 0.30439, Accuracy: 92.21% | Test loss: 0.75652, AUC: 0.8625209224179116, MCC: 0.42836506907641037, sum: 4820.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss: 0.31330, Accuracy: 92.99% | Test loss: 0.82546, AUC: 0.8575241791173323, MCC: 0.4279725126091989, sum: 4071.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 0.05964, Accuracy: 91.12% | Test loss: 0.80640, AUC: 0.8614457391683057, MCC: 0.4056095856099756, sum: 5528.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    109\u001b[39m loss.backward()\n\u001b[32m    111\u001b[39m optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m batch_losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.float().detach().numpy())\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n\u001b[32m    116\u001b[39m gc.collect()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = EsmForTokenClassificationCustom().to(DEVICE)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 12\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('esm'): # choose whatever you like here\n",
    "        param.requires_grad = False\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "\n",
    "    if epoch > 2:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(input_ids, attention_mask)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "            # print(output)\n",
    "\n",
    "            logits = output.flatten(1)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            # print(output.logits.shape, flattened_labels.shape)\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]# .float()\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(valid_flattened_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "        output = model(input_ids, attention_mask)\n",
    "        \n",
    "        logits = output.flatten(1)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 650M without LORA - here:\n",
    "# Epoch: 1 | Loss: 0.26357, Accuracy: 88.84% | Test loss: 0.59217, AUC: 0.8772442293540929, MCC: 0.3847349478773821, sum: 7178.0\n",
    "# \n",
    "# 650M with LORA - here:\n",
    "# Epoch: 3 | Loss: 0.93123, Accuracy: 86.97% | Test loss: 0.63380, AUC: 0.8560885121193611, MCC: 0.3594092927832279, sum: 8330.0\n",
    "#\n",
    "# 650M without LORA - custom-finetuning.ipynb:\n",
    "# Epoch: 1 | Loss: 0.34253, Accuracy: 91.27% | Test loss: 0.59555, AUC: 0.878478909678348, MCC: 0.41902639123765595, sum: 5562.0\n",
    "# 5CxR1K6pCkVasM6LyXl8Vs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 2560])\n"
     ]
    }
   ],
   "source": [
    "from esm.sdk.forge import ESM3ForgeInferenceClient\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "\n",
    "protein = ESMProtein(sequence=\"AAAAA\")\n",
    "# Apply for forge access and get an access token\n",
    "forge_client = ESM3ForgeInferenceClient(model=\"esmc-6b-2024-12\", url=\"https://forge.evolutionaryscale.ai\", token=\"5CxR1K6pCkVasM6LyXl8Vs\")\n",
    "protein_tensor = forge_client.encode(protein)\n",
    "logits_output = forge_client.logits(\n",
    "   protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    ")\n",
    "print(logits_output.embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1a4uB.txt ... 1/946\n",
      "Processing 1a8dA.txt ... 2/946\n",
      "Processing 1ad1A.txt ... 3/946\n",
      "Processing 1ak1A.txt ... 4/946\n",
      "Processing 1arlA.txt ... 5/946\n",
      "Processing 1aylA.txt ... 6/946\n",
      "Processing 1b0iA.txt ... 7/946\n",
      "Processing 1bfnA.txt ... 8/946\n",
      "Processing 1bhsA.txt ... 9/946\n",
      "Processing 1bk2A.txt ... 10/946\n",
      "Processing 1byiA.txt ... 11/946\n",
      "Processing 1bzjA.txt ... 12/946\n",
      "Processing 1c3kA.txt ... 13/946\n",
      "Processing 1cuzA.txt ... 14/946\n",
      "Processing 1cwqA.txt ... 15/946\n",
      "Processing 1dc6A.txt ... 16/946\n",
      "Processing 1dklA.txt ... 17/946\n",
      "Processing 1dpjA.txt ... 18/946\n",
      "Processing 1dq2A.txt ... 19/946\n",
      "Processing 1dqzA.txt ... 20/946\n",
      "Processing 1dteA.txt ... 21/946\n",
      "Processing 1e3gA.txt ... 22/946\n",
      "Processing 1e5lB.txt ... 23/946\n",
      "Processing 1e6kA.txt ... 24/946\n",
      "Processing 1eccB.txt ... 25/946\n",
      "Processing 1efhB.txt ... 26/946\n",
      "Processing 1eswA.txt ... 27/946\n",
      "Processing 1evyA.txt ... 28/946\n",
      "Processing 1ezlC.txt ... 29/946\n",
      "Processing 1f47B.txt ... 30/946\n",
      "Processing 1f8aB.txt ... 31/946\n",
      "Processing 1fd9A.txt ... 32/946\n",
      "Processing 1fdpA.txt ... 33/946\n",
      "Processing 1ffhA.txt ... 34/946\n",
      "Processing 1fl1B.txt ... 35/946\n",
      "Processing 1fvrA.txt ... 36/946\n",
      "Processing 1fwkC.txt ... 37/946\n",
      "Processing 1g24C.txt ... 38/946\n",
      "Processing 1g59A.txt ... 39/946\n",
      "Processing 1gqnA.txt ... 40/946\n",
      "Processing 1gqzA.txt ... 41/946\n",
      "Processing 1h13A.txt ... 42/946\n",
      "Processing 1h3gB.txt ... 43/946\n",
      "Processing 1havA.txt ... 44/946\n",
      "Processing 1hbqA.txt ... 45/946\n",
      "Processing 1hp1A.txt ... 46/946\n",
      "Processing 1ht6A.txt ... 47/946\n",
      "Processing 1i0rB.txt ... 48/946\n",
      "Processing 1i7nA.txt ... 49/946\n",
      "Processing 1iwlA.txt ... 50/946\n",
      "Processing 1j8fC.txt ... 51/946\n",
      "Processing 1jpmA.txt ... 52/946\n",
      "Processing 1k0nA.txt ... 53/946\n",
      "Processing 1k1xB.txt ... 54/946\n",
      "Processing 1k47D.txt ... 55/946\n",
      "Processing 1k4kD.txt ... 56/946\n",
      "Processing 1k7kA.txt ... 57/946\n",
      "Processing 1kg5A.txt ... 58/946\n",
      "Processing 1kn9D.txt ... 59/946\n",
      "Processing 1ks9A.txt ... 60/946\n",
      "Processing 1ksgB.txt ... 61/946\n",
      "Processing 1kx9A.txt ... 62/946\n",
      "Processing 1kxrA.txt ... 63/946\n",
      "Processing 1l0wB.txt ... 64/946\n",
      "Processing 1lbeB.txt ... 65/946\n",
      "Processing 1ljuA.txt ... 66/946\n",
      "Processing 1lugA.txt ... 67/946\n",
      "Processing 1m1zA.txt ... 68/946\n",
      "Processing 1m5wD.txt ... 69/946\n",
      "Processing 1macB.txt ... 70/946\n",
      "Processing 1mhnA.txt ... 71/946\n",
      "Processing 1ms4B.txt ... 72/946\n",
      "Processing 1mufA.txt ... 73/946\n",
      "Processing 1mwkA.txt ... 74/946\n",
      "Processing 1mwrA.txt ... 75/946\n",
      "Processing 1n05A.txt ... 76/946\n",
      "Processing 1nawB.txt ... 77/946\n",
      "Processing 1nbfB.txt ... 78/946\n",
      "Processing 1nd7A.txt ... 79/946\n",
      "Processing 1nkoA.txt ... 80/946\n",
      "Processing 1nn6A.txt ... 81/946\n",
      "Processing 1nokA.txt ... 82/946\n",
      "Processing 1nulB.txt ... 83/946\n",
      "Processing 1nwhB.txt ... 84/946\n",
      "Processing 1nzoA.txt ... 85/946\n",
      "Processing 1o73A.txt ... 86/946\n",
      "Processing 1of3B.txt ... 87/946\n",
      "Processing 1oibB.txt ... 88/946\n",
      "Processing 1omxA.txt ... 89/946\n",
      "Processing 1os2B.txt ... 90/946\n",
      "Processing 1p4oB.txt ... 91/946\n",
      "Processing 1p4vA.txt ... 92/946\n",
      "Processing 1p74A.txt ... 93/946\n",
      "Processing 1p9oA.txt ... 94/946\n",
      "Processing 1pfzC.txt ... 95/946\n",
      "Processing 1pt7A.txt ... 96/946\n",
      "Processing 1ptaA.txt ... 97/946\n",
      "Processing 1pu5C.txt ... 98/946\n",
      "Processing 1px5B.txt ... 99/946\n",
      "Processing 1py3C.txt ... 100/946\n",
      "Processing 1q4kA.txt ... 101/946\n",
      "Processing 1qhtA.txt ... 102/946\n",
      "Processing 1qrzC.txt ... 103/946\n",
      "Processing 1r8jA.txt ... 104/946\n",
      "Processing 1rf5D.txt ... 105/946\n",
      "Processing 1rjbA.txt ... 106/946\n",
      "Processing 1rkmA.txt ... 107/946\n",
      "Processing 1rq2B.txt ... 108/946\n",
      "Processing 1rtcA.txt ... 109/946\n",
      "Processing 1rxdC.txt ... 110/946\n",
      "Processing 1s2lC.txt ... 111/946\n",
      "Processing 1s8cC.txt ... 112/946\n",
      "Processing 1se8A.txt ... 113/946\n",
      "Processing 1sh0A.txt ... 114/946\n",
      "Processing 1sjsA.txt ... 115/946\n",
      "Processing 1sndB.txt ... 116/946\n",
      "Processing 1sulA.txt ... 117/946\n",
      "Processing 1t8tA.txt ... 118/946\n",
      "Processing 1t9rA.txt ... 119/946\n",
      "Processing 1thvA.txt ... 120/946\n",
      "Processing 1tplB.txt ... 121/946\n",
      "Processing 1tqdA.txt ... 122/946\n",
      "Processing 1tqnA.txt ... 123/946\n",
      "Processing 1u4pA.txt ... 124/946\n",
      "Processing 1uiuA.txt ... 125/946\n",
      "Processing 1ukaA.txt ... 126/946\n",
      "Processing 1un1B.txt ... 127/946\n",
      "Processing 1urpC.txt ... 128/946\n",
      "Processing 1uteA.txt ... 129/946\n",
      "Processing 1vjuB.txt ... 130/946\n",
      "Processing 1vk4A.txt ... 131/946\n",
      "Processing 1vr2A.txt ... 132/946\n",
      "Processing 1vsnA.txt ... 133/946\n",
      "Processing 1wamA.txt ... 134/946\n",
      "Processing 1wjgA.txt ... 135/946\n",
      "Processing 1wxeA.txt ... 136/946\n",
      "Processing 1wycA.txt ... 137/946\n",
      "Processing 1x0mA.txt ... 138/946\n",
      "Processing 1x2gC.txt ... 139/946\n",
      "Processing 1xgdA.txt ... 140/946\n",
      "Processing 1xhxB.txt ... 141/946\n",
      "Processing 1xjfA.txt ... 142/946\n",
      "Processing 1xqvA.txt ... 143/946\n",
      "Processing 1xqzA.txt ... 144/946\n",
      "Processing 1xt3B.txt ... 145/946\n",
      "Processing 1xtcA.txt ... 146/946\n",
      "Processing 1y6iA.txt ... 147/946\n",
      "Processing 1yhvA.txt ... 148/946\n",
      "Processing 1yl5B.txt ... 149/946\n",
      "Processing 1ys0A.txt ... 150/946\n",
      "Processing 1z7gD.txt ... 151/946\n",
      "Processing 1z90B.txt ... 152/946\n",
      "Processing 1zm0A.txt ... 153/946\n",
      "Processing 2a88A.txt ... 154/946\n",
      "Processing 2airH.txt ... 155/946\n",
      "Processing 2akaA.txt ... 156/946\n",
      "Processing 2akrA.txt ... 157/946\n",
      "Processing 2b0jA.txt ... 158/946\n",
      "Processing 2b23B.txt ... 159/946\n",
      "Processing 2b7cA.txt ... 160/946\n",
      "Processing 2beiB.txt ... 161/946\n",
      "Processing 2bivB.txt ... 162/946\n",
      "Processing 2bvaA.txt ... 163/946\n",
      "Processing 2by3A.txt ... 164/946\n",
      "Processing 2c3vA.txt ... 165/946\n",
      "Processing 2c6gA.txt ... 166/946\n",
      "Processing 2camA.txt ... 167/946\n",
      "Retrying... Attempt 1 after 1.0s due to: (429, 'Failure in encode: {\"status\":\"error\",\"message\":\"You have exceeded your daily credit limit of 10 credits.\"}')\n",
      "Retrying... Attempt 2 after 2.0s due to: (429, 'Failure in encode: {\"status\":\"error\",\"message\":\"You have exceeded your daily credit limit of 10 credits.\"}')\n",
      "Retrying... Attempt 3 after 4.0s due to: (429, 'Failure in encode: {\"status\":\"error\",\"message\":\"You have exceeded your daily credit limit of 10 credits.\"}')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m sequence = f.read()\n\u001b[32m     16\u001b[39m protein = ESMProtein(sequence=sequence)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m protein_tensor = \u001b[43mforge_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m logits_output = forge_client.logits(\n\u001b[32m     19\u001b[39m    protein_tensor, LogitsConfig(sequence=\u001b[38;5;28;01mTrue\u001b[39;00m, return_embeddings=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m vectors = logits_output.embeddings.cpu().numpy()[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/esm/sdk/forge.py:243\u001b[39m, in \u001b[36mESM3ForgeInferenceClient.retry_decorator.<locals>.wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m retry_decorator = retry(\n\u001b[32m    233\u001b[39m     retry=retry_if_result(retry_if_specific_error),\n\u001b[32m    234\u001b[39m     wait=wait_exponential(\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     before_sleep=log_retry_attempt,\n\u001b[32m    241\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Apply the retry decorator to the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/tenacity/__init__.py:487\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[32m    486\u001b[39m     retry_state.prepare_for_next_attempt()\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/tenacity/nap.py:31\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_path = '/home/skrhakv/cryptic-nn/data/cryptobench/sequences'\n",
    "output_dir = '/home/skrhakv/cryptic-nn/data/cryptobench/ESMC-6B-embeddings'\n",
    "files_list = os.listdir(f'{input_path}')\n",
    "i = 0\n",
    "for filename in files_list:\n",
    "    i = i + 1\n",
    "    name, ext = os.path.splitext(filename)\n",
    "\n",
    "    print(f\"Processing {filename} ... {i}/{len(files_list)}\")\n",
    "    if os.path.isfile(f'{output_dir}/{name}.npy'):\n",
    "        continue\n",
    "    with open(f'{input_path}/{filename}', 'r') as f:\n",
    "        sequence = f.read()\n",
    "        protein = ESMProtein(sequence=sequence)\n",
    "        protein_tensor = forge_client.encode(protein)\n",
    "        logits_output = forge_client.logits(\n",
    "           protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    "        )\n",
    "        vectors = logits_output.embeddings.cpu().numpy()[0][1:-1]\n",
    "        # print(vectors.shape)\n",
    "        np.save(f'{output_dir}/{name}.npy', vectors)    \n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESMC_DIM = 1152\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "class EsmForTokenClassificationCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.esm = ESMC.from_pretrained(\"esmc_600m\")\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(ESMC_DIM, OUTPUT_SIZE)\n",
    "        self.plDDT_regressor = nn.Linear(ESMC_DIM, OUTPUT_SIZE)\n",
    "        self.distance_regressor = nn.Linear(ESMC_DIM, OUTPUT_SIZE)\n",
    "        self.tokenizer = self.esm.tokenizer\n",
    "\n",
    "    def forward(self, input, sequence_id):\n",
    "        outputs = self.esm(input, sequence_id).embeddings.float()\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs1 = self.classifier(outputs)\n",
    "        outputs2 = self.plDDT_regressor(outputs)\n",
    "        outputs3 = self.distance_regressor(outputs)\n",
    "        return outputs1, outputs2, outputs3\n",
    "\n",
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "def get_dataset_with_distances(annotation_path, tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    distances = []\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "            \n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distance = np.clip(distance, 0, 10)\n",
    "\n",
    "            if len(distance) != len(sequence): \n",
    "                print(f'{protein_id} doesn\\'t match. Skipping ...')\n",
    "                break\n",
    "\n",
    "            # scale the distance\n",
    "            distance = scaler.transform(distance.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "            distances.append(distance)\n",
    "    train_tokenized = tokenizer(sequences) #, padding='max_length', truncation=True, max_length=MAX_LENGTH)# , max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    dataset = dataset.add_column(\"distances\", distances)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def train_scaler(annotation_path, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "    distances = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distances.append(distance)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.concatenate(distances).reshape(-1, 1))\n",
    "    return scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForTokenClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.35426, Accuracy: 73.84% | Test loss: 1.23588 - CBS: 0.99393, DIST: 0.24195194244384766, AUC: 0.5582355, MCC: 0.04280, sum: 13368.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.26860, Accuracy: 85.71% | Test loss: 0.65426 - CBS: 0.62432, DIST: 0.02993944101035595, AUC: 0.8627819, MCC: 0.38973, sum: 9689.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.41235, Accuracy: 78.27% | Test loss: 0.68616 - CBS: 0.66904, DIST: 0.017126796767115593, AUC: 0.8774177, MCC: 0.33023, sum: 14250.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.32506, Accuracy: 87.64% | Test loss: 0.60706 - CBS: 0.59648, DIST: 0.010579128749668598, AUC: 0.8752388, MCC: 0.40874, sum: 8447.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "from esm.models.esmc import ESMC\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # controls class imbalance\n",
    "        self.gamma = gamma  # focuses on hard examples\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate Binary Cross-Entropy Loss for each sample\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Compute pt (model confidence on true class)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        # Apply the focal adjustment\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        # Apply reduction (mean, sum, or no reduction)\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('Synthyra/ESMplusplus_large', num_labels=3).to(DEVICE)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# train only the classifier for the first few epochs \n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('transformer'): # choose whatever you like here\n",
    "        param.requires_grad = False\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn) \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = EsmForTokenClassificationCustom().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 4\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # unfreeze and do end-to-end finetuning after training only the classifier for the first few epochs \n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            cbs_logits, distance_logits, _ = model(input_ids, attention_mask)\n",
    "                        \n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            # probabilities = torch.sigmoid(valid_flattened_cbs_logits)\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits)) # out = (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "\n",
    "            test_loss = cbs_test_loss + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        cbs_logits, distance_logits, _ = model(input_ids, attention_mask)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "        \n",
    "        # different loss, sigmoid\n",
    "        loss = cbs_loss + distances_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss:.5f}, DIST: {distances_test_loss}, AUC: {roc_auc:.7f}, MCC: {mcc:.5f}, sum: {sum(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.70342, Accuracy: 87.83% | Test loss: 0.65470 - CBS: 0.65470, DIST: 0.2378130555152893, AUC: 0.8682842, MCC: 0.40147, sum: 8201.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.35642, Accuracy: 90.90% | Test loss: 0.54783 - CBS: 0.54783, DIST: 0.030145596712827682, AUC: 0.8946092, MCC: 0.46367, sum: 6454.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.19164, Accuracy: 91.14% | Test loss: 0.59060 - CBS: 0.59060, DIST: 0.016224149614572525, AUC: 0.8908447, MCC: 0.44764, sum: 6036.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.14579, Accuracy: 93.19% | Test loss: 0.73728 - CBS: 0.73728, DIST: 0.009741359390318394, AUC: 0.8791066, MCC: 0.43978, sum: 4006.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.06521, Accuracy: 91.68% | Test loss: 0.77536 - CBS: 0.77536, DIST: 0.008904056623578072, AUC: 0.8806394, MCC: 0.42975, sum: 5330.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# different loss, sigmoid\u001b[39;00m\n\u001b[32m    109\u001b[39m loss = cbs_loss + distances_loss\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m loss.backward()\n\u001b[32m    114\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/optim/optimizer.py:975\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[32m--> \u001b[39m\u001b[32m975\u001b[39m         p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    977\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt')\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer, scaler)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# similarly train only the classifier for the first few epochs \n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('transformer') or name.startswith('classifier.0'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EPOCHS = 8\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # unfreeze and do end-to-end finetuning after training only the classifier for the first few epochs \n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            cbs_logits, _, distance_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            # probabilities = torch.sigmoid(valid_flattened_cbs_logits)\n",
    "            predictions = out = torch.round(torch.sigmoid(valid_flattened_cbs_logits)) # (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "\n",
    "            test_loss = cbs_test_loss # + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        cbs_logits, _, distance_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "        \n",
    "        # different loss, sigmoid\n",
    "        loss = cbs_loss + distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss:.5f}, DIST: {distances_test_loss}, AUC: {roc_auc:.7f}, MCC: {mcc:.5f}, sum: {sum(predictions)}\")\n",
    "\n",
    "# ligysis - freezing first 3 epochs, then 3 epochs full; CryptoBench - freezing first 3 epochs, then full:\n",
    "# Epoch: 3 | Loss: 0.51589, Accuracy: 89.64% | Test loss: 0.57979 - CBS: 0.57979, AUC: 0.8856013, MCC: 0.42233, sum: 7002.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneESM(nn.Module):\n",
    "    def __init__(self, esm_model: str) -> None:\n",
    "        super().__init__()\n",
    "        self.llm = EsmModel.from_pretrained(esm_model) #, torch_dtype=torch.bfloat16)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE) # , dtype=torch.bfloat16)\n",
    "        self.plDDT_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE) # , dtype=torch.bfloat16)\n",
    "        self.distance_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE) # , dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(self, batch: dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        token_embeddings = self.llm(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        return self.classifier(token_embeddings), self.plDDT_regressor(token_embeddings), self.distance_regressor(token_embeddings)\n",
    "\n",
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "def get_dataset_with_distances(annotation_path, tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    distances = []\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "            \n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distance = np.clip(distance, 0, 10)\n",
    "\n",
    "            if len(distance) != len(sequence): \n",
    "                print(f'{protein_id} doesn\\'t match. Skipping ...')\n",
    "                break\n",
    "\n",
    "            # scale the distance\n",
    "            distance = scaler.transform(distance.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "            distances.append(distance)\n",
    "    train_tokenized = tokenizer(sequences) #, padding='max_length', truncation=True, max_length=MAX_LENGTH)# , max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    dataset = dataset.add_column(\"distances\", distances)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def train_scaler(annotation_path, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "    distances = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distances.append(distance)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.concatenate(distances).reshape(-1, 1))\n",
    "    return scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0b6e24b4794c8db7e1ed0f74a52dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = FinetuneESM(MODEL_NAME).half().to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "partial_collate_fn = functools.partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11735.0625 MB / 81090.125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.76957, Accuracy: 33.33% | Test loss: 1.25038, AUC: 0.48524422031609643, MCC: -0.018371569854482916, sum: 37991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.77076, Accuracy: 83.03% | Test loss: 0.79982, AUC: 0.8054946088230709, MCC: 0.27954933155679573, sum: 10219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 1.13276, Accuracy: 79.51% | Test loss: 0.77825, AUC: 0.8220818877271572, MCC: 0.2845631026768609, sum: 12761\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import gc\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=0.0001, eps=1e-4) \n",
    "# optimizer = torch.optim.AdamW8bit(params=model.parameters(),\n",
    "#                             lr=0.0001)\n",
    "EPOCHS = 3\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('llm'): \n",
    "        param.requires_grad = False\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # input_ids = batch['input_ids'].to(DEVICE)\n",
    "            # attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output1, output2, _ = model(batch)\n",
    "\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            distances = batch['distances'].to(DEVICE)\n",
    "    \n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "            distance_logits = output2.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(cbs_logits)) # (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  plDDT_loss_fn(torch.sigmoid(distance_logits), valid_flattened_distances.float())\n",
    "\n",
    "            test_loss = cbs_test_loss + distances_test_loss\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, distance_logits, cbs_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        output1, output2, _ = model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        distances = batch['distances'].to(DEVICE)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "        distance_logits = output2.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(distance_logits), valid_flattened_distances.half())\n",
    "\n",
    "        loss = cbs_loss + distances_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, output1, output2, cbs_logits, distance_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions.to(dtype=torch.int))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.34115, Accuracy: 54.50% | Test loss: 1.27098, AUC: 0.8837, MCC: 0.2081, F1: 0.6557, AUPRC: 0.4335, sum: 27988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.83689, Accuracy: 54.50% | Test loss: 0.98234, AUC: 0.8837, MCC: 0.2081, F1: 0.6557, AUPRC: 0.4335, sum: 27988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.22976, Accuracy: 54.50% | Test loss: 0.98080, AUC: 0.8837, MCC: 0.2081, F1: 0.6557, AUPRC: 0.4335, sum: 27988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.39914, Accuracy: 91.54% | Test loss: 0.54185, AUC: 0.8966, MCC: 0.4642, F1: 0.9270, AUPRC: 0.4944, sum: 5890\n"
     ]
    }
   ],
   "source": [
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt')\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer, scaler)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "# similarly train only the classifier for the first few epochs \n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('llm') or name.startswith('classifier'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "\n",
    "            output1, _, output3 = model(batch)\n",
    "\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            distances = batch['distances'].to(DEVICE)\n",
    "    \n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "            distance_logits = output3.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(cbs_logits)) # (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  plDDT_loss_fn(torch.sigmoid(distance_logits), valid_flattened_distances.float())\n",
    "\n",
    "            test_loss = cbs_test_loss + distances_test_loss\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "\n",
    "            f1 = metrics.f1_score(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "            precision, recall, thresholds = metrics.precision_recall_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "            auprc = metrics.auc(recall, precision)\n",
    "\n",
    "            del labels, distance_logits, cbs_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        output1, _, output3 = model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        distances = batch['distances'].to(DEVICE)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "        distance_logits = output3.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(distance_logits), valid_flattened_distances.half())\n",
    "\n",
    "        loss = cbs_loss + distances_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, output1, output3, cbs_logits, distance_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(predictions.to(dtype=torch.int))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
    "  warnings.warn(\n",
    "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
    "  warnings.warn(\n",
    "Epoch: 0 | Loss: 1.24023, Accuracy: 60.46% | Test loss: 1.14751, AUC: 0.8812185281680878, MCC: 0.23148245180101992, sum: 24565\n",
    "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
    "  warnings.warn(\n",
    "Epoch: 1 | Loss: 0.76695, Accuracy: 60.46% | Test loss: 0.90364, AUC: 0.8812185281680878, MCC: 0.23148245180101992, sum: 24565\n",
    "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
    "  warnings.warn(\n",
    "Epoch: 2 | Loss: 0.22735, Accuracy: 60.46% | Test loss: 0.90216, AUC: 0.8812185281680878, MCC: 0.23148245180101992, sum: 24565\n",
    "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
    "  warnings.warn(\n",
    "Epoch: 3 | Loss: 0.39764, Accuracy: 91.87% | Test loss: 0.54452, AUC: 0.8959868619672968, MCC: 0.47038283510266105, sum: 5670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5670, device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions.to(dtype=torch.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'r', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9501953\n",
      "0.05319822672577581\n",
      "0.0007059182661121074\n"
     ]
    }
   ],
   "source": [
    "print(thresholds[62])\n",
    "print(tpr[62])\n",
    "print(fpr[62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the predictions for the test set and do some further analysis locally ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = '/home/skrhakv/cryptic-nn/data/cryptobench/test.txt'\n",
    "\n",
    "def predict(sequence):\n",
    "    sequence = 'MDLTNKNVIFVAALGGIGLDTSRELVKRNLKNFVILDRVENPTALAELKAINPKVNITFHTYDVTVPVAESKKLLKKIFDQLKTVDILINGAGILDDHQIERTIAINFTGLVNTTTAILDFWDKRKGGPGGIIANICSVTGFNAIHQVPVYSASKAAVVSFTNSLAKLAPITGVTAYSINPGITRTPLVHTFNSWLDVEPRVAELLLSHPTQTSEQCGQNFVKAIEANKNGAIWKLDLGTLEAIEWTKHWDSHI'\n",
    "    tokenized_sequences = tokenizer(sequence, max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "    # convert to tensor\n",
    "    tokenized_sequences = {k: torch.tensor([v]).to(DEVICE) for k,v in tokenized_sequences.items()}\n",
    "    output, _, _ = model(tokenized_sequences)\n",
    "    output = output.flatten()\n",
    "\n",
    "    mask = (tokenized_sequences['attention_mask'] == 1).flatten()\n",
    "\n",
    "    output = torch.sigmoid(output[mask][1:-1]).detach().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "with open(TEST_PATH) as f:\n",
    "    reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "    for row in reader:\n",
    "        sequence = row[4]\n",
    "        # max sequence length of ESM2\n",
    "        if len(sequence) > MAX_LENGTH: continue \n",
    "\n",
    "        indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "        label = np.zeros(len(sequence)) #, dtype=np.float16)\n",
    "        label[indices] = 1\n",
    "        prediction = predict(sequence)\n",
    "\n",
    "        assert len(sequence) == len(label)\n",
    "\n",
    "        id = row[0] + row[1]\n",
    "        np.save(f'/home/skrhakv/cryptic-nn/src/fine-tuning/predictions/predictions/{id}.npy', prediction)\n",
    "        np.save(f'/home/skrhakv/cryptic-nn/src/fine-tuning/predictions/ground-truth/{id}.npy', label)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
