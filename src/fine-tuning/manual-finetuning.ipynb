{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc337d598a7b4495a7004aceb081cfe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EsmModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import csv\n",
    "from datasets import Dataset\n",
    "import functools\n",
    "\n",
    "DROPOUT = 0.25\n",
    "OUTPUT_SIZE = 1\n",
    "MAX_LENGTH = 1024\n",
    "LABEL_PAD_TOKEN_ID = -100\n",
    "\n",
    "# MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
    "# MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "MODEL_NAME = 'facebook/esm2_t36_3B_UR50D'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def print_used_memory():\n",
    "    free, total = torch.cuda.mem_get_info(torch.device('cuda:0'))\n",
    "    mem_used_MB = (total - free) / 1024 ** 2\n",
    "    mem_total_MB = (total) / 1024 ** 2\n",
    "    print(f'{mem_used_MB} MB / {mem_total_MB} MB')\n",
    "\n",
    "class FinetuneESM(nn.Module):\n",
    "    def __init__(self, esm_model: str) -> None:\n",
    "        super().__init__()\n",
    "        self.llm = EsmModel.from_pretrained(esm_model) #, torch_dtype=torch.bfloat16)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE) # , dtype=torch.bfloat16)\n",
    "        \n",
    "    def forward(self, batch: dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        token_embeddings = self.llm(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "        return self.classifier(token_embeddings)\n",
    "\n",
    "def get_dataset(annotation_path, tokenizer):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "\n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence)) #, dtype=np.float16)\n",
    "            label[indices] = 1\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "\n",
    "            assert len(sequence) == len(label)\n",
    "\n",
    "    tokenized_sequences = tokenizer(sequences, max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "\n",
    "    dataset = Dataset.from_dict(tokenized_sequences)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "\n",
    "    if \"distances\" in batch[0].keys():\n",
    "        label_names = ['labels', 'distances']\n",
    "    else:\n",
    "        label_names = ['labels']\n",
    "\n",
    "    labels = {label_name: [feature[label_name] for feature in batch] for label_name in label_names}\n",
    "    no_labels_features = [{k: v for k, v in feature.items() if k not in label_names} for feature in batch]\n",
    "\n",
    "    batch = tokenizer.pad(\n",
    "        no_labels_features,\n",
    "        padding=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    # type to bfloat16\n",
    "    # batch['input_ids'] = batch['input_ids'].to(dtype=torch.float16).to(DEVICE)\n",
    "    # batch['attention_mask'] = batch['attention_mask'].to(dtype=torch.float16).to(DEVICE)\n",
    "\n",
    "    sequence_length = batch[\"input_ids\"].shape[1]\n",
    "\n",
    "    for label_name in label_names:\n",
    "        batch[label_name] = [[LABEL_PAD_TOKEN_ID] + list(label) + [LABEL_PAD_TOKEN_ID] * (sequence_length - len(label)-1) for label in labels[label_name]]\n",
    "        batch[label_name] = torch.tensor(batch[label_name]).to(DEVICE)# , dtype=torch.bfloat16).to(DEVICE)\n",
    "\n",
    "    return batch\n",
    "\n",
    "model = FinetuneESM(MODEL_NAME).half().to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "partial_collate_fn = functools.partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "# train_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer)\n",
    "# val_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "# \n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=partial_collate_fn)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train end-to-end without lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6097.0625 MB / 81090.125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.93886, Accuracy: 75.37% | Test loss: 0.98541, AUC: 0.4966444777008342, MCC: 0.00851241159477756, sum: 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.41030, Accuracy: 86.53% | Test loss: 0.58455, AUC: 0.8831857513908667, MCC: 0.38747096053283003, sum: 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.16598, Accuracy: 89.82% | Test loss: 0.56725, AUC: 0.8894166977418276, MCC: 0.4210427297022199, sum: 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 1.01018, Accuracy: 91.55% | Test loss: 0.68505, AUC: 0.8826773671146894, MCC: 0.424533219608909, sum: 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.93870, Accuracy: 94.32% | Test loss: 0.94482, AUC: 0.5987813101629393, MCC: 0.0, sum: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1524b8ac7950>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[32m     91\u001b[39m optimizer.zero_grad()\n\u001b[32m     93\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m batch_losses.append(loss.cpu().float().detach().numpy())\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py:292\u001b[39m, in \u001b[36mOptimizer8bit.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.prefetch_state(p)\n\u001b[32m    291\u001b[39m         \u001b[38;5;28mself\u001b[39m.update_step(group, p, gindex, pindex)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_paged:\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# all paged operation are asynchronous, we need\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;66;03m# to sync to make sure all tensors are in the right state\u001b[39;00m\n\u001b[32m    296\u001b[39m     torch.cuda.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/cuda/__init__.py:985\u001b[39m, in \u001b[36msynchronize\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    983\u001b[39m _lazy_init()\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.device(device):\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import gc\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=0.0001, eps=1e-4) \n",
    "# optimizer = torch.optim.AdamW8bit(params=model.parameters(),\n",
    "#                             lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # input_ids = batch['input_ids'].to(DEVICE)\n",
    "            # attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(batch)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "            logits = output.flatten(1)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]# .float()\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(valid_flattened_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        output = model(batch) #, attention_mask=attention_mask)\n",
    "        logits = output.flatten(1)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 850M without LORA - here:\n",
    "# Epoch: 1 | Loss: 0.26357, Accuracy: 88.84% | Test loss: 0.59217, AUC: 0.8772442293540929, MCC: 0.3847349478773821, sum: 7178.0\n",
    "# \n",
    "# 850M without LORA - custom-finetuning.ipynb:\n",
    "# Epoch: 1 | Loss: 0.34253, Accuracy: 91.27% | Test loss: 0.59555, AUC: 0.878478909678348, MCC: 0.41902639123765595, sum: 5562.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Any other classifier head needs to be added here! (in future: apo-holo distance/plDDT regresor)\n",
      "3081.0625 MB / 81090.125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.07641, Accuracy: 84.94% | Test loss: 0.96781, AUC: 0.5584380776297676, MCC: 0.06338426312302034, sum: 6467.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.98292, Accuracy: 90.08% | Test loss: 0.70329, AUC: 0.828017489151323, MCC: 0.33694228067099186, sum: 5599.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.28898, Accuracy: 94.32% | Test loss: 0.92707, AUC: 0.5952360896233821, MCC: 0.0, sum: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.93123, Accuracy: 86.97% | Test loss: 0.63380, AUC: 0.8560885121193611, MCC: 0.3594092927832279, sum: 8330.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.35971, Accuracy: 86.21% | Test loss: 0.64358, AUC: 0.8543168563973295, MCC: 0.3511116215049817, sum: 8806.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 0.20516, Accuracy: 89.37% | Test loss: 0.70072, AUC: 0.8532096263216228, MCC: 0.3716428972575048, sum: 6588.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/deeplife/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 101\u001b[0m\n\u001b[1;32m     97\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 101\u001b[0m batch_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n\u001b[1;32m    104\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/skrhakv/cryptic-nn/src/fine-tuning')\n",
    "\n",
    "from lora import apply_lora # , freeze_all_but_head\n",
    "\n",
    "apply_lora(model) # .to(DEVICE)\n",
    "\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # input_ids = batch['input_ids'].to(DEVICE)\n",
    "            # attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(batch)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "            logits = output.flatten(1)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]# .float()\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(valid_flattened_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        output = model(batch) #, attention_mask=attention_mask)\n",
    "        logits = output.flatten(1)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 850M without LORA - here:\n",
    "# Epoch: 1 | Loss: 0.26357, Accuracy: 88.84% | Test loss: 0.59217, AUC: 0.8772442293540929, MCC: 0.3847349478773821, sum: 7178.0\n",
    "# \n",
    "# 850M with LORA - here:\n",
    "# Epoch: 3 | Loss: 0.93123, Accuracy: 86.97% | Test loss: 0.63380, AUC: 0.8560885121193611, MCC: 0.3594092927832279, sum: 8330.0\n",
    "#\n",
    "# 850M without LORA - custom-finetuning.ipynb:\n",
    "# Epoch: 1 | Loss: 0.34253, Accuracy: 91.27% | Test loss: 0.59555, AUC: 0.878478909678348, MCC: 0.41902639123765595, sum: 5562.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForTokenClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained('Synthyra/ESMplusplus_large', num_labels=1, problem_type='single_label_classification').to(DEVICE)\n",
    "tokenizer = model.tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esm++ tryout .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18529.0625 MB / 81090.125 MB\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([0.1934], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.80839, Accuracy: 19.61% | Test loss: 1.76119, AUC: 0.4258982469501052, MCC: -0.07413211318761029, sum: 46029.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([0.0760], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.38677, Accuracy: 82.28% | Test loss: 0.62088, AUC: 0.8778704881941413, MCC: 0.3450996010979654, sum: 11621.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-0.1264], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.09270, Accuracy: 88.66% | Test loss: 0.66985, AUC: 0.8738272595540745, MCC: 0.3814881770277971, sum: 7282.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-0.7474], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.19809, Accuracy: 92.79% | Test loss: 0.98229, AUC: 0.8650654748465431, MCC: 0.4265198775537588, sum: 4246.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-1.1127], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.03363, Accuracy: 90.89% | Test loss: 1.24782, AUC: 0.8564089380902804, MCC: 0.38666894885946734, sum: 5505.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-0.6319], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 0.04672, Accuracy: 91.43% | Test loss: 1.16187, AUC: 0.8604517740366541, MCC: 0.4091554389917905, sum: 5298.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-0.9140], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss: 0.05581, Accuracy: 92.45% | Test loss: 1.34929, AUC: 0.8511572368989107, MCC: 0.39652730919182694, sum: 4219.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-1.0599], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss: 0.00296, Accuracy: 92.82% | Test loss: 1.95489, AUC: 0.8424755979534786, MCC: 0.3872959655490615, sum: 3764.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-1.2158], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss: 0.06317, Accuracy: 92.07% | Test loss: 1.72772, AUC: 0.8482709805426195, MCC: 0.3931933804662767, sum: 4532.0\n",
      "torch.Size([184, 1024, 1]) torch.Size([188416])\n",
      "tensor([-1.6969], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss: 0.00201, Accuracy: 92.85% | Test loss: 2.02877, AUC: 0.8289615701010321, MCC: 0.36783836976154755, sum: 3526.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "partial_collate_fn = functools.partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "train_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer)\n",
    "val_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "            # print(output)\n",
    "            logits = output.logits.flatten(1)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            # print(output.logits.shape, flattened_labels.shape)\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]# .float()\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(valid_flattened_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask) #, attention_mask=attention_mask)\n",
    "        logits = output.logits.flatten(1)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 850M without LORA - here:\n",
    "# Epoch: 1 | Loss: 0.26357, Accuracy: 88.84% | Test loss: 0.59217, AUC: 0.8772442293540929, MCC: 0.3847349478773821, sum: 7178.0\n",
    "# \n",
    "# 850M with LORA - here:\n",
    "# Epoch: 3 | Loss: 0.93123, Accuracy: 86.97% | Test loss: 0.63380, AUC: 0.8560885121193611, MCC: 0.3594092927832279, sum: 8330.0\n",
    "#\n",
    "# 850M without LORA - custom-finetuning.ipynb:\n",
    "# Epoch: 1 | Loss: 0.34253, Accuracy: 91.27% | Test loss: 0.59555, AUC: 0.878478909678348, MCC: 0.41902639123765595, sum: 5562.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "def get_dataset_with_distances(annotation_path, tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    distances = []\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "            \n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distance = np.clip(distance, 0, 10)\n",
    "\n",
    "            if len(distance) != len(sequence): \n",
    "                print(f'{protein_id} doesn\\'t match. Skipping ...')\n",
    "                break\n",
    "\n",
    "            # scale the distance\n",
    "            distance = scaler.transform(distance.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "            distances.append(distance)\n",
    "    train_tokenized = tokenizer(sequences) #, padding='max_length', truncation=True, max_length=MAX_LENGTH)# , max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    dataset = dataset.add_column(\"distances\", distances)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def train_scaler(annotation_path, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "    distances = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distances.append(distance)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.concatenate(distances).reshape(-1, 1))\n",
    "    return scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForTokenClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.89152, Accuracy: 54.79% | Test loss: 1.08529 - CBS: 1.08529, AUC: 0.6218540, MCC: 0.08392, sum: 25996.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.53873, Accuracy: 83.67% | Test loss: 0.71629 - CBS: 0.71629, AUC: 0.8344590, MCC: 0.30053, sum: 10030.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.63747, Accuracy: 75.40% | Test loss: 0.76744 - CBS: 0.76744, AUC: 0.8411284, MCC: 0.27558, sum: 15445.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    120\u001b[39m loss = cbs_loss +  distances_loss\n\u001b[32m    121\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m optimizer.step()\n\u001b[32m    127\u001b[39m batch_losses.append(loss.cpu().detach().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained('Synthyra/ESMplusplus_large', num_labels=3).to(DEVICE)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# train only the classifier for the first few epochs \n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('transformer'): # choose whatever you like here\n",
    "        param.requires_grad = False\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn) \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 4\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # unfreeze and do end-to-end finetuning after training only the classifier for the first few epochs \n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            cbs_logits = output.logits[:,:,0]\n",
    "            distance_logits = output.logits[:,:,1]\n",
    "            \n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            # distances_test_loss =  distances_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "\n",
    "            test_loss = cbs_test_loss # + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cbs_logits = output.logits[:,:,0]\n",
    "        distance_logits = output.logits[:,:,1]\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "        \n",
    "        # different loss, sigmoid\n",
    "        loss = cbs_loss +  distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss:.5f}, AUC: {roc_auc:.7f}, MCC: {mcc:.5f}, sum: {sum(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.45650, Accuracy: 90.52% | Test loss: 0.81983 - CBS: 0.81983, AUC: 0.8717390, MCC: 0.42576, sum: 6302.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.11850, Accuracy: 89.87% | Test loss: 0.94013 - CBS: 0.94013, AUC: 0.8638283, MCC: 0.40362, sum: 6570.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    114\u001b[39m loss = cbs_loss +  distances_loss\n\u001b[32m    115\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m optimizer.step()\n\u001b[32m    121\u001b[39m batch_losses.append(loss.cpu().detach().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/venv_esmplusplus/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt')\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer, scaler)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# similarly train only the classifier for the first few epochs \n",
    "# for name, param in model.named_parameters():\n",
    "#      if name.startswith('transformer'): # choose whatever you like here\n",
    "#         param.requires_grad = False\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EPOCHS = 8\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # unfreeze and do end-to-end finetuning after training only the classifier for the first few epochs \n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            cbs_logits = output.logits[:,:,0]\n",
    "            distance_logits = output.logits[:,:,2]\n",
    "            \n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            # distances_test_loss =  distances_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances)\n",
    "\n",
    "            test_loss = cbs_test_loss # + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cbs_logits = output.logits[:,:,0]\n",
    "        distance_logits = output.logits[:,:,2]\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "        \n",
    "        # different loss, sigmoid\n",
    "        loss = cbs_loss +  distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss:.5f}, AUC: {roc_auc:.7f}, MCC: {mcc:.5f}, sum: {sum(predictions)}\")\n",
    "\n",
    "# ligysis - freezing first 3 epochs, then 3 epochs full; CryptoBench - freezing first 3 epochs, then full:\n",
    "# Epoch: 3 | Loss: 0.51589, Accuracy: 89.64% | Test loss: 0.57979 - CBS: 0.57979, AUC: 0.8856013, MCC: 0.42233, sum: 7002.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emsC tryout ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "\n",
    "def get_dataset(annotation_path, tokenizer):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "\n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence)) #, dtype=np.float16)\n",
    "            label[indices] = 1\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "\n",
    "            assert len(sequence) == len(label)\n",
    "\n",
    "    tokenized_sequences = tokenizer(sequences, max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "\n",
    "    dataset = Dataset.from_dict(tokenized_sequences)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "ESMC_DIM = 1152\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "class EsmForTokenClassificationCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.esm = ESMC.from_pretrained(\"esmc_600m\")\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(ESMC_DIM, OUTPUT_SIZE)\n",
    "        self.tokenizer = self.esm.tokenizer\n",
    "\n",
    "    def forward(self, input, sequence_id):\n",
    "        outputs = self.esm(input, sequence_id).embeddings.float()\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.classifier(outputs)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForTokenClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0f6707e1ff4a2a9c2cc8a384fb0607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('Synthyra/ESMplusplus_large', num_labels=3).to(DEVICE)\n",
    "tokenizer = model.tokenizer\n",
    "model = EsmForTokenClassificationCustom()\n",
    "\n",
    "\n",
    "train_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', model.tokenizer)\n",
    "val_dataset = get_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', model.tokenizer)\n",
    "\n",
    "partial_collate_fn = functools.partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15981.0625 MB / 81090.125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.11461, Accuracy: 81.13% | Test loss: 0.99226, AUC: 0.5952734257314609, MCC: 0.05722997355599492, sum: 8874.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.98699, Accuracy: 93.82% | Test loss: 0.96832, AUC: 0.7470172014095504, MCC: 0.17029190371498892, sum: 955.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.74901, Accuracy: 93.79% | Test loss: 0.94789, AUC: 0.7685537711124515, MCC: 0.19150300902632247, sum: 1111.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.96995, Accuracy: 93.58% | Test loss: 0.92987, AUC: 0.777330043196548, MCC: 0.20147752062808624, sum: 1366.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.38523, Accuracy: 88.93% | Test loss: 0.60264, AUC: 0.8757627772921935, MCC: 0.41047776514573, sum: 7433.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 0.16317, Accuracy: 89.85% | Test loss: 0.59487, AUC: 0.8734679599282306, MCC: 0.4145183582869328, sum: 6728.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss: 0.36034, Accuracy: 91.64% | Test loss: 0.65518, AUC: 0.8691317395028082, MCC: 0.42025814196812533, sum: 5246.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss: 0.14920, Accuracy: 92.15% | Test loss: 0.70656, AUC: 0.86906248948034, MCC: 0.4257054828357001, sum: 4843.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss: 0.30439, Accuracy: 92.21% | Test loss: 0.75652, AUC: 0.8625209224179116, MCC: 0.42836506907641037, sum: 4820.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss: 0.31330, Accuracy: 92.99% | Test loss: 0.82546, AUC: 0.8575241791173323, MCC: 0.4279725126091989, sum: 4071.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 0.05964, Accuracy: 91.12% | Test loss: 0.80640, AUC: 0.8614457391683057, MCC: 0.4056095856099756, sum: 5528.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    109\u001b[39m loss.backward()\n\u001b[32m    111\u001b[39m optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m batch_losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.float().detach().numpy())\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n\u001b[32m    116\u001b[39m gc.collect()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = EsmForTokenClassificationCustom().to(DEVICE)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 12\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('esm'): # choose whatever you like here\n",
    "        param.requires_grad = False\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "\n",
    "    if epoch > 2:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output = model(input_ids, attention_mask)\n",
    "            # print('after prediction:')\n",
    "            # print_used_memory()\n",
    "            # print(output)\n",
    "\n",
    "            logits = output.flatten(1)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            # print(output.logits.shape, flattened_labels.shape)\n",
    "            valid_flattened_logits = logits.flatten()[flattened_labels != -100]# .float()\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_logits))\n",
    "\n",
    "            test_loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "            # print(valid_flattened_logits)\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(valid_flattened_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "        output = model(input_ids, attention_mask)\n",
    "        \n",
    "        logits = output.flatten(1)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        flattened_labels = labels.flatten()\n",
    "        # print_used_memory()\n",
    "\n",
    "        valid_flattened_logits = logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss = loss_fn(valid_flattened_logits, valid_flattened_labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, logits, valid_flattened_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n",
    "# 650M without LORA - here:\n",
    "# Epoch: 1 | Loss: 0.26357, Accuracy: 88.84% | Test loss: 0.59217, AUC: 0.8772442293540929, MCC: 0.3847349478773821, sum: 7178.0\n",
    "# \n",
    "# 650M with LORA - here:\n",
    "# Epoch: 3 | Loss: 0.93123, Accuracy: 86.97% | Test loss: 0.63380, AUC: 0.8560885121193611, MCC: 0.3594092927832279, sum: 8330.0\n",
    "#\n",
    "# 650M without LORA - custom-finetuning.ipynb:\n",
    "# Epoch: 1 | Loss: 0.34253, Accuracy: 91.27% | Test loss: 0.59555, AUC: 0.878478909678348, MCC: 0.41902639123765595, sum: 5562.0\n",
    "# 5CxR1K6pCkVasM6LyXl8Vs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 2560])\n"
     ]
    }
   ],
   "source": [
    "from esm.sdk.forge import ESM3ForgeInferenceClient\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "\n",
    "protein = ESMProtein(sequence=\"AAAAA\")\n",
    "# Apply for forge access and get an access token\n",
    "forge_client = ESM3ForgeInferenceClient(model=\"esmc-6b-2024-12\", url=\"https://forge.evolutionaryscale.ai\", token=\"5CxR1K6pCkVasM6LyXl8Vs\")\n",
    "protein_tensor = forge_client.encode(protein)\n",
    "logits_output = forge_client.logits(\n",
    "   protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    ")\n",
    "print(logits_output.embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1a4uB.txt ... 1/946\n",
      "Processing 1a8dA.txt ... 2/946\n",
      "Processing 1ad1A.txt ... 3/946\n",
      "Processing 1ak1A.txt ... 4/946\n",
      "Processing 1arlA.txt ... 5/946\n",
      "Processing 1aylA.txt ... 6/946\n",
      "Processing 1b0iA.txt ... 7/946\n",
      "Processing 1bfnA.txt ... 8/946\n",
      "Processing 1bhsA.txt ... 9/946\n",
      "Processing 1bk2A.txt ... 10/946\n",
      "Processing 1byiA.txt ... 11/946\n",
      "Processing 1bzjA.txt ... 12/946\n",
      "Processing 1c3kA.txt ... 13/946\n",
      "Processing 1cuzA.txt ... 14/946\n",
      "Processing 1cwqA.txt ... 15/946\n",
      "Processing 1dc6A.txt ... 16/946\n",
      "Processing 1dklA.txt ... 17/946\n",
      "Processing 1dpjA.txt ... 18/946\n",
      "Processing 1dq2A.txt ... 19/946\n",
      "Processing 1dqzA.txt ... 20/946\n",
      "Processing 1dteA.txt ... 21/946\n",
      "Processing 1e3gA.txt ... 22/946\n",
      "Processing 1e5lB.txt ... 23/946\n",
      "Processing 1e6kA.txt ... 24/946\n",
      "Processing 1eccB.txt ... 25/946\n",
      "Processing 1efhB.txt ... 26/946\n",
      "Processing 1eswA.txt ... 27/946\n",
      "Processing 1evyA.txt ... 28/946\n",
      "Processing 1ezlC.txt ... 29/946\n",
      "Processing 1f47B.txt ... 30/946\n",
      "Processing 1f8aB.txt ... 31/946\n",
      "Processing 1fd9A.txt ... 32/946\n",
      "Processing 1fdpA.txt ... 33/946\n",
      "Processing 1ffhA.txt ... 34/946\n",
      "Processing 1fl1B.txt ... 35/946\n",
      "Processing 1fvrA.txt ... 36/946\n",
      "Processing 1fwkC.txt ... 37/946\n",
      "Processing 1g24C.txt ... 38/946\n",
      "Processing 1g59A.txt ... 39/946\n",
      "Processing 1gqnA.txt ... 40/946\n",
      "Processing 1gqzA.txt ... 41/946\n",
      "Processing 1h13A.txt ... 42/946\n",
      "Processing 1h3gB.txt ... 43/946\n",
      "Processing 1havA.txt ... 44/946\n",
      "Processing 1hbqA.txt ... 45/946\n",
      "Processing 1hp1A.txt ... 46/946\n",
      "Processing 1ht6A.txt ... 47/946\n",
      "Processing 1i0rB.txt ... 48/946\n",
      "Processing 1i7nA.txt ... 49/946\n",
      "Processing 1iwlA.txt ... 50/946\n",
      "Processing 1j8fC.txt ... 51/946\n",
      "Processing 1jpmA.txt ... 52/946\n",
      "Processing 1k0nA.txt ... 53/946\n",
      "Processing 1k1xB.txt ... 54/946\n",
      "Processing 1k47D.txt ... 55/946\n",
      "Processing 1k4kD.txt ... 56/946\n",
      "Processing 1k7kA.txt ... 57/946\n",
      "Processing 1kg5A.txt ... 58/946\n",
      "Processing 1kn9D.txt ... 59/946\n",
      "Processing 1ks9A.txt ... 60/946\n",
      "Processing 1ksgB.txt ... 61/946\n",
      "Processing 1kx9A.txt ... 62/946\n",
      "Processing 1kxrA.txt ... 63/946\n",
      "Processing 1l0wB.txt ... 64/946\n",
      "Processing 1lbeB.txt ... 65/946\n",
      "Processing 1ljuA.txt ... 66/946\n",
      "Processing 1lugA.txt ... 67/946\n",
      "Processing 1m1zA.txt ... 68/946\n",
      "Processing 1m5wD.txt ... 69/946\n",
      "Processing 1macB.txt ... 70/946\n",
      "Processing 1mhnA.txt ... 71/946\n",
      "Processing 1ms4B.txt ... 72/946\n",
      "Processing 1mufA.txt ... 73/946\n",
      "Processing 1mwkA.txt ... 74/946\n",
      "Processing 1mwrA.txt ... 75/946\n",
      "Processing 1n05A.txt ... 76/946\n",
      "Processing 1nawB.txt ... 77/946\n",
      "Processing 1nbfB.txt ... 78/946\n",
      "Processing 1nd7A.txt ... 79/946\n",
      "Processing 1nkoA.txt ... 80/946\n",
      "Processing 1nn6A.txt ... 81/946\n",
      "Processing 1nokA.txt ... 82/946\n",
      "Processing 1nulB.txt ... 83/946\n",
      "Processing 1nwhB.txt ... 84/946\n",
      "Processing 1nzoA.txt ... 85/946\n",
      "Processing 1o73A.txt ... 86/946\n",
      "Processing 1of3B.txt ... 87/946\n",
      "Processing 1oibB.txt ... 88/946\n",
      "Processing 1omxA.txt ... 89/946\n",
      "Processing 1os2B.txt ... 90/946\n",
      "Processing 1p4oB.txt ... 91/946\n",
      "Processing 1p4vA.txt ... 92/946\n",
      "Processing 1p74A.txt ... 93/946\n",
      "Processing 1p9oA.txt ... 94/946\n",
      "Processing 1pfzC.txt ... 95/946\n",
      "Processing 1pt7A.txt ... 96/946\n",
      "Processing 1ptaA.txt ... 97/946\n",
      "Processing 1pu5C.txt ... 98/946\n",
      "Processing 1px5B.txt ... 99/946\n",
      "Processing 1py3C.txt ... 100/946\n",
      "Processing 1q4kA.txt ... 101/946\n",
      "Processing 1qhtA.txt ... 102/946\n",
      "Processing 1qrzC.txt ... 103/946\n",
      "Processing 1r8jA.txt ... 104/946\n",
      "Processing 1rf5D.txt ... 105/946\n",
      "Processing 1rjbA.txt ... 106/946\n",
      "Processing 1rkmA.txt ... 107/946\n",
      "Processing 1rq2B.txt ... 108/946\n",
      "Processing 1rtcA.txt ... 109/946\n",
      "Processing 1rxdC.txt ... 110/946\n",
      "Processing 1s2lC.txt ... 111/946\n",
      "Processing 1s8cC.txt ... 112/946\n",
      "Processing 1se8A.txt ... 113/946\n",
      "Processing 1sh0A.txt ... 114/946\n",
      "Processing 1sjsA.txt ... 115/946\n",
      "Processing 1sndB.txt ... 116/946\n",
      "Processing 1sulA.txt ... 117/946\n",
      "Processing 1t8tA.txt ... 118/946\n",
      "Processing 1t9rA.txt ... 119/946\n",
      "Processing 1thvA.txt ... 120/946\n",
      "Processing 1tplB.txt ... 121/946\n",
      "Processing 1tqdA.txt ... 122/946\n",
      "Processing 1tqnA.txt ... 123/946\n",
      "Processing 1u4pA.txt ... 124/946\n",
      "Processing 1uiuA.txt ... 125/946\n",
      "Processing 1ukaA.txt ... 126/946\n",
      "Processing 1un1B.txt ... 127/946\n",
      "Processing 1urpC.txt ... 128/946\n",
      "Processing 1uteA.txt ... 129/946\n",
      "Processing 1vjuB.txt ... 130/946\n",
      "Processing 1vk4A.txt ... 131/946\n",
      "Processing 1vr2A.txt ... 132/946\n",
      "Processing 1vsnA.txt ... 133/946\n",
      "Processing 1wamA.txt ... 134/946\n",
      "Processing 1wjgA.txt ... 135/946\n",
      "Processing 1wxeA.txt ... 136/946\n",
      "Processing 1wycA.txt ... 137/946\n",
      "Processing 1x0mA.txt ... 138/946\n",
      "Processing 1x2gC.txt ... 139/946\n",
      "Processing 1xgdA.txt ... 140/946\n",
      "Processing 1xhxB.txt ... 141/946\n",
      "Processing 1xjfA.txt ... 142/946\n",
      "Processing 1xqvA.txt ... 143/946\n",
      "Processing 1xqzA.txt ... 144/946\n",
      "Processing 1xt3B.txt ... 145/946\n",
      "Processing 1xtcA.txt ... 146/946\n",
      "Processing 1y6iA.txt ... 147/946\n",
      "Processing 1yhvA.txt ... 148/946\n",
      "Processing 1yl5B.txt ... 149/946\n",
      "Processing 1ys0A.txt ... 150/946\n",
      "Processing 1z7gD.txt ... 151/946\n",
      "Processing 1z90B.txt ... 152/946\n",
      "Processing 1zm0A.txt ... 153/946\n",
      "Processing 2a88A.txt ... 154/946\n",
      "Processing 2airH.txt ... 155/946\n",
      "Processing 2akaA.txt ... 156/946\n",
      "Processing 2akrA.txt ... 157/946\n",
      "Processing 2b0jA.txt ... 158/946\n",
      "Processing 2b23B.txt ... 159/946\n",
      "Processing 2b7cA.txt ... 160/946\n",
      "Processing 2beiB.txt ... 161/946\n",
      "Processing 2bivB.txt ... 162/946\n",
      "Processing 2bvaA.txt ... 163/946\n",
      "Processing 2by3A.txt ... 164/946\n",
      "Processing 2c3vA.txt ... 165/946\n",
      "Processing 2c6gA.txt ... 166/946\n",
      "Processing 2camA.txt ... 167/946\n",
      "Retrying... Attempt 1 after 1.0s due to: (429, 'Failure in encode: {\"status\":\"error\",\"message\":\"You have exceeded your daily credit limit of 10 credits.\"}')\n",
      "Retrying... Attempt 2 after 2.0s due to: (429, 'Failure in encode: {\"status\":\"error\",\"message\":\"You have exceeded your daily credit limit of 10 credits.\"}')\n",
      "Retrying... Attempt 3 after 4.0s due to: (429, 'Failure in encode: {\"status\":\"error\",\"message\":\"You have exceeded your daily credit limit of 10 credits.\"}')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m sequence = f.read()\n\u001b[32m     16\u001b[39m protein = ESMProtein(sequence=sequence)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m protein_tensor = \u001b[43mforge_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m logits_output = forge_client.logits(\n\u001b[32m     19\u001b[39m    protein_tensor, LogitsConfig(sequence=\u001b[38;5;28;01mTrue\u001b[39;00m, return_embeddings=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m vectors = logits_output.embeddings.cpu().numpy()[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/esm/sdk/forge.py:243\u001b[39m, in \u001b[36mESM3ForgeInferenceClient.retry_decorator.<locals>.wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m retry_decorator = retry(\n\u001b[32m    233\u001b[39m     retry=retry_if_result(retry_if_specific_error),\n\u001b[32m    234\u001b[39m     wait=wait_exponential(\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     before_sleep=log_retry_attempt,\n\u001b[32m    241\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Apply the retry decorator to the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/tenacity/__init__.py:487\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[32m    486\u001b[39m     retry_state.prepare_for_next_attempt()\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/tenacity/nap.py:31\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(seconds)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_path = '/home/skrhakv/cryptic-nn/data/cryptobench/sequences'\n",
    "output_dir = '/home/skrhakv/cryptic-nn/data/cryptobench/ESMC-6B-embeddings'\n",
    "files_list = os.listdir(f'{input_path}')\n",
    "i = 0\n",
    "for filename in files_list:\n",
    "    i = i + 1\n",
    "    name, ext = os.path.splitext(filename)\n",
    "\n",
    "    print(f\"Processing {filename} ... {i}/{len(files_list)}\")\n",
    "    if os.path.isfile(f'{output_dir}/{name}.npy'):\n",
    "        continue\n",
    "    with open(f'{input_path}/{filename}', 'r') as f:\n",
    "        sequence = f.read()\n",
    "        protein = ESMProtein(sequence=sequence)\n",
    "        protein_tensor = forge_client.encode(protein)\n",
    "        logits_output = forge_client.logits(\n",
    "           protein_tensor, LogitsConfig(sequence=True, return_embeddings=True)\n",
    "        )\n",
    "        vectors = logits_output.embeddings.cpu().numpy()[0][1:-1]\n",
    "        # print(vectors.shape)\n",
    "        np.save(f'{output_dir}/{name}.npy', vectors)    \n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESMC_DIM = 1152\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "class EsmForTokenClassificationCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.esm = ESMC.from_pretrained(\"esmc_600m\")\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(ESMC_DIM, OUTPUT_SIZE)\n",
    "        self.plDDT_regressor = nn.Linear(ESMC_DIM, OUTPUT_SIZE)\n",
    "        self.distance_regressor = nn.Linear(ESMC_DIM, OUTPUT_SIZE)\n",
    "        self.tokenizer = self.esm.tokenizer\n",
    "\n",
    "    def forward(self, input, sequence_id):\n",
    "        outputs = self.esm(input, sequence_id).embeddings.float()\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs1 = self.classifier(outputs)\n",
    "        outputs2 = self.plDDT_regressor(outputs)\n",
    "        outputs3 = self.distance_regressor(outputs)\n",
    "        return outputs1, outputs2, outputs3\n",
    "\n",
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "def get_dataset_with_distances(annotation_path, tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    distances = []\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "            \n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distance = np.clip(distance, 0, 10)\n",
    "\n",
    "            if len(distance) != len(sequence): \n",
    "                print(f'{protein_id} doesn\\'t match. Skipping ...')\n",
    "                break\n",
    "\n",
    "            # scale the distance\n",
    "            distance = scaler.transform(distance.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "            distances.append(distance)\n",
    "    train_tokenized = tokenizer(sequences) #, padding='max_length', truncation=True, max_length=MAX_LENGTH)# , max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    dataset = dataset.add_column(\"distances\", distances)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def train_scaler(annotation_path, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "    distances = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distances.append(distance)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.concatenate(distances).reshape(-1, 1))\n",
    "    return scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ESMplusplusForTokenClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.35426, Accuracy: 73.84% | Test loss: 1.23588 - CBS: 0.99393, DIST: 0.24195194244384766, AUC: 0.5582355, MCC: 0.04280, sum: 13368.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.26860, Accuracy: 85.71% | Test loss: 0.65426 - CBS: 0.62432, DIST: 0.02993944101035595, AUC: 0.8627819, MCC: 0.38973, sum: 9689.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.41235, Accuracy: 78.27% | Test loss: 0.68616 - CBS: 0.66904, DIST: 0.017126796767115593, AUC: 0.8774177, MCC: 0.33023, sum: 14250.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.32506, Accuracy: 87.64% | Test loss: 0.60706 - CBS: 0.59648, DIST: 0.010579128749668598, AUC: 0.8752388, MCC: 0.40874, sum: 8447.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "from esm.models.esmc import ESMC\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # controls class imbalance\n",
    "        self.gamma = gamma  # focuses on hard examples\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate Binary Cross-Entropy Loss for each sample\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Compute pt (model confidence on true class)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        # Apply the focal adjustment\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        # Apply reduction (mean, sum, or no reduction)\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('Synthyra/ESMplusplus_large', num_labels=3).to(DEVICE)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# train only the classifier for the first few epochs \n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('transformer'): # choose whatever you like here\n",
    "        param.requires_grad = False\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn) \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = EsmForTokenClassificationCustom().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                            lr=0.0001)\n",
    "EPOCHS = 4\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # unfreeze and do end-to-end finetuning after training only the classifier for the first few epochs \n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            cbs_logits, distance_logits, _ = model(input_ids, attention_mask)\n",
    "                        \n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            # probabilities = torch.sigmoid(valid_flattened_cbs_logits)\n",
    "            predictions = torch.round(torch.sigmoid(valid_flattened_cbs_logits)) # out = (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "\n",
    "            test_loss = cbs_test_loss + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        cbs_logits, distance_logits, _ = model(input_ids, attention_mask)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "        \n",
    "        # different loss, sigmoid\n",
    "        loss = cbs_loss + distances_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss:.5f}, DIST: {distances_test_loss}, AUC: {roc_auc:.7f}, MCC: {mcc:.5f}, sum: {sum(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.70342, Accuracy: 87.83% | Test loss: 0.65470 - CBS: 0.65470, DIST: 0.2378130555152893, AUC: 0.8682842, MCC: 0.40147, sum: 8201.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.35642, Accuracy: 90.90% | Test loss: 0.54783 - CBS: 0.54783, DIST: 0.030145596712827682, AUC: 0.8946092, MCC: 0.46367, sum: 6454.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.19164, Accuracy: 91.14% | Test loss: 0.59060 - CBS: 0.59060, DIST: 0.016224149614572525, AUC: 0.8908447, MCC: 0.44764, sum: 6036.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.14579, Accuracy: 93.19% | Test loss: 0.73728 - CBS: 0.73728, DIST: 0.009741359390318394, AUC: 0.8791066, MCC: 0.43978, sum: 4006.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 0.06521, Accuracy: 91.68% | Test loss: 0.77536 - CBS: 0.77536, DIST: 0.008904056623578072, AUC: 0.8806394, MCC: 0.42975, sum: 5330.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# different loss, sigmoid\u001b[39;00m\n\u001b[32m    109\u001b[39m loss = cbs_loss + distances_loss\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m loss.backward()\n\u001b[32m    114\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/optim/optimizer.py:975\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[32m--> \u001b[39m\u001b[32m975\u001b[39m         p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    977\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt')\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer, scaler)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# similarly train only the classifier for the first few epochs \n",
    "for name, param in model.named_parameters():\n",
    "     if name.startswith('transformer') or name.startswith('classifier.0'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "EPOCHS = 8\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # unfreeze and do end-to-end finetuning after training only the classifier for the first few epochs \n",
    "    if epoch > 1:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            cbs_logits, _, distance_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            labels = batch['labels'].to(device)\n",
    "            distances = batch['distances'].to(device)\n",
    "\n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            # probabilities = torch.sigmoid(valid_flattened_cbs_logits)\n",
    "            predictions = out = torch.round(torch.sigmoid(valid_flattened_cbs_logits)) # (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "\n",
    "            test_loss = cbs_test_loss # + distances_test_loss\n",
    "            test_losses.append(test_loss.cpu().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "\n",
    "            # print(torch.sum(torch.isnan(torch.sigmoid(valid_flattened_cbs_logits))))\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().numpy(), torch.sigmoid(valid_flattened_cbs_logits).cpu().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # Padded labels from the data collator\n",
    "        # padded_labels += batch['labels'].tolist()\n",
    "\n",
    "        cbs_logits, _, distance_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "        distances = batch['distances'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        valid_flattened_cbs_logits = cbs_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distance_logits = distance_logits.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(valid_flattened_cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(valid_flattened_distance_logits), valid_flattened_distances.float())\n",
    "        \n",
    "        # different loss, sigmoid\n",
    "        loss = cbs_loss + distances_loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f} - CBS: {cbs_test_loss:.5f}, DIST: {distances_test_loss}, AUC: {roc_auc:.7f}, MCC: {mcc:.5f}, sum: {sum(predictions)}\")\n",
    "\n",
    "# ligysis - freezing first 3 epochs, then 3 epochs full; CryptoBench - freezing first 3 epochs, then full:\n",
    "# Epoch: 3 | Loss: 0.51589, Accuracy: 89.64% | Test loss: 0.57979 - CBS: 0.57979, AUC: 0.8856013, MCC: 0.42233, sum: 7002.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneESM(nn.Module):\n",
    "    def __init__(self, esm_model: str) -> None:\n",
    "        super().__init__()\n",
    "        self.llm = EsmModel.from_pretrained(esm_model) #, torch_dtype=torch.bfloat16)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE) # , dtype=torch.bfloat16)\n",
    "        self.plDDT_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE) # , dtype=torch.bfloat16)\n",
    "        self.distance_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE) # , dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(self, batch: dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        token_embeddings = self.llm(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        return self.classifier(token_embeddings), self.plDDT_regressor(token_embeddings), self.distance_regressor(token_embeddings)\n",
    "\n",
    "from datasets import Dataset\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "def get_dataset_with_distances(annotation_path, tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    distances = []\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            sequence = row[4]\n",
    "            # max sequence length of ESM2\n",
    "            if len(sequence) > MAX_LENGTH: continue \n",
    "            \n",
    "            indices = [int(residue[1:]) for residue in row[3].split(' ')]\n",
    "            label = np.zeros(len(sequence))\n",
    "            label[indices] = 1\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distance = np.clip(distance, 0, 10)\n",
    "\n",
    "            if len(distance) != len(sequence): \n",
    "                print(f'{protein_id} doesn\\'t match. Skipping ...')\n",
    "                break\n",
    "\n",
    "            # scale the distance\n",
    "            distance = scaler.transform(distance.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label) # np.eye(NUMBER_OF_CLASSES)[label])\n",
    "            distances.append(distance)\n",
    "    train_tokenized = tokenizer(sequences) #, padding='max_length', truncation=True, max_length=MAX_LENGTH)# , max_length=MAX_LENGTH, padding=True, truncation=True)\n",
    "    \n",
    "    dataset = Dataset.from_dict(train_tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    dataset = dataset.add_column(\"distances\", distances)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def train_scaler(annotation_path, distances_path='/home/skrhakv/cryptic-nn/data/cryptobench/residue-distances', uniprot_ids=False):\n",
    "    distances = []\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "\n",
    "        for row in reader:\n",
    "            if not uniprot_ids:\n",
    "                protein_id = row[0].lower() + row[1]\n",
    "            else:\n",
    "                protein_id = row[0]\n",
    "            distance = np.load(f'{distances_path}/{protein_id}.npy')\n",
    "            distance[distance == -1] = 0.5\n",
    "            distances.append(distance)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(np.concatenate(distances).reshape(-1, 1))\n",
    "    return scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f94e00a64914fb584055caa04cc96f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = FinetuneESM(MODEL_NAME).half().to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "partial_collate_fn = functools.partial(collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "scaler = train_scaler('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "train_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', tokenizer, scaler, distances_path='/home/skrhakv/cryptic-nn/data/ligysis/plDDT', uniprot_ids=True)\n",
    "val_dataset = get_dataset_with_distances('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer, scaler)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17981.0625 MB / 81090.125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skrhakv/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import gc\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print_used_memory()\n",
    "\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=0.0001, eps=1e-4) \n",
    "# optimizer = torch.optim.AdamW8bit(params=model.parameters(),\n",
    "#                             lr=0.0001)\n",
    "EPOCHS = 10\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "class_weights = torch.tensor([0.5303, 8.7481], device='cuda:0')\n",
    "\n",
    "cbs_loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "plDDT_loss_fn = nn.MSELoss() \n",
    "\n",
    "# TODO: Try multiply the class_weights[1] * 2\n",
    "# BCEWithLogitsLoss - sigmoid is already built-in!\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    # print('Before test:')\n",
    "    # print_used_memory()\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # input_ids = batch['input_ids'].to(DEVICE)\n",
    "            # attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            # print('before prediction:')\n",
    "            # print_used_memory()\n",
    "\n",
    "            output1, output2, _ = model(batch)\n",
    "\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            distances = batch['distances'].to(DEVICE)\n",
    "    \n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "            distance_logits = output2.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "            predictions = torch.round(torch.sigmoid(cbs_logits)) # (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "            cbs_test_loss =  cbs_loss_fn(cbs_logits, valid_flattened_labels)\n",
    "            distances_test_loss =  plDDT_loss_fn(torch.sigmoid(distance_logits), valid_flattened_distances.float())\n",
    "\n",
    "            test_loss = cbs_test_loss + distances_test_loss\n",
    "\n",
    "            test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "\n",
    "            # compute metrics on test dataset\n",
    "            test_acc = accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                   y_pred=predictions)\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "            del labels, distance_logits, cbs_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # print('after test')\n",
    "    # print_used_memory()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "\n",
    "    # TODO: the following row causes the memory explosion\n",
    "    # with torch.inference_mode():\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        output1, output2, _ = model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        distances = batch['distances'].to(DEVICE)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "        distance_logits = output2.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_distances = distances.flatten()[flattened_labels != -100]\n",
    "\n",
    "        predictions = torch.round(torch.sigmoid(cbs_logits)) # (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "        cbs_loss =  cbs_loss_fn(cbs_logits, valid_flattened_labels)\n",
    "        distances_loss =  plDDT_loss_fn(torch.sigmoid(distance_logits), valid_flattened_distances.half())\n",
    "\n",
    "        loss = cbs_loss + distances_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, output1, output2, cbs_logits, distance_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc}, MCC: {mcc}, sum: {sum(predictions)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166152"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "184*903"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
