{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9258be4",
   "metadata": {},
   "source": [
    "# Pocket-centric metrics\n",
    "Metrics like F1, MCC, ACC, AUC ROC, and AUPRC are oriented towards binary classification. However, during the binding site prediction, pockets are the actual target. Let's perform an analysis oriented towards pockets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78c13ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "27\n",
      "26\n",
      "29\n",
      "28\n",
      "30\n",
      "20\n",
      "3\n",
      "No pockets found; N= 1, len(clustered_centroids) = 0\n",
      "21\n",
      "30\n",
      "29\n",
      "40\n",
      "13\n",
      "23\n",
      "14\n",
      "31\n",
      "31\n",
      "3\n",
      "No pockets found; N= 1, len(clustered_centroids) = 0\n",
      "25\n",
      "11\n",
      "17\n",
      "1\n",
      "No pockets found; N= 1, len(clustered_centroids) = 0\n",
      "12\n",
      "55\n",
      "1\n",
      "No pockets found; N= 3, len(clustered_centroids) = 0\n",
      "24\n",
      "21\n",
      "21\n",
      "21\n",
      "24\n",
      "29\n",
      "12\n",
      "14\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 3)) while a minimum of 1 is required by DBSCAN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 177\u001b[0m\n\u001b[1;32m    173\u001b[0m         total_pockets_found \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pockets_found\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_pockets, total_pockets_found, total_pockets_found \u001b[38;5;241m/\u001b[39m total_pockets\n\u001b[0;32m--> 177\u001b[0m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 153\u001b[0m, in \u001b[0;36manalyze\u001b[0;34m(metric)\u001b[0m\n\u001b[1;32m    151\u001b[0m predicted_binding_scores \u001b[38;5;241m=\u001b[39m predictions[predictions_mask]\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(predicted_binding_residues))\n\u001b[0;32m--> 153\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_binding_residues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredictions_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m clusters_centroids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# loop over each cluster and compute the cluster's center with its average score\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/cryptic-nn/src/prediction-analysis/clustering.py:27\u001b[0m, in \u001b[0;36mcompute_clusters\u001b[0;34m(points, prediction_scores, check_sasa)\u001b[0m\n\u001b[1;32m     25\u001b[0m dbscan \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39mEPSILON, min_samples\u001b[38;5;241m=\u001b[39mMIN_SAMPLES)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# dbscan = AgglomerativeClustering(distance_threshold=EPSILON, n_clusters=None, linkage='single')\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mdbscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhigh_score_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Initialize all labels to -1\u001b[39;00m\n\u001b[1;32m     30\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(points), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/cluster/_dbscan.py:474\u001b[0m, in \u001b[0;36mDBSCAN.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/cluster/_dbscan.py:395\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# DBSCAN.metric is not validated yet\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    369\u001b[0m )\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform DBSCAN clustering from features, or distance matrix.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m        Returns a fitted instance of self.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m         sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1091\u001b[0m         )\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 3)) while a minimum of 1 is required by DBSCAN."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "from biotite.structure import get_residue_starts, Atom, AtomArray\n",
    "from biotite.structure.io.pdbx import get_structure\n",
    "import biotite.structure.io.pdbx as pdbx\n",
    "import biotite.database.rcsb as rcsb\n",
    "import biotite\n",
    "\n",
    "from clustering import compute_clusters, THRESHOLD\n",
    "\n",
    "DATASET_PATH = '/home/vit/Projects/cryptic-nn/datasets/cryptobench-dataset/folds/test.json'\n",
    "CIF_FILES = '/home/vit/Projects/deeplife-project/data/cif_files'\n",
    "PREDICTIONS_PATH = '/home/vit/Projects/cryptic-nn/data/predictions/ESM2-3B-extended-finetuning'\n",
    "K = 2\n",
    "\n",
    "def load_dataset():\n",
    "    with open(DATASET_PATH, 'r') as json_file:\n",
    "        dataset = json.load(json_file)\n",
    "    return dataset\n",
    "\n",
    "def load_structure(pdb_id):\n",
    "    cif_file_path = rcsb.fetch(pdb_id, \"cif\", target_path=CIF_FILES)\n",
    "    cif_file = pdbx.CIFFile.read(cif_file_path)\n",
    "    return get_structure(cif_file, model=1)\n",
    "\n",
    "def compute_centroid(pocket_residues):\n",
    "    pocket_coords = np.array([residue.coord for residue in pocket_residues])\n",
    "    pocket_center = np.mean(pocket_coords, axis=0)\n",
    "    return pocket_center\n",
    "\n",
    "def compute_pocket_center(structure, pocket):\n",
    "    pocket_residues = [residue for residue in structure if str(residue.res_id) in pocket]\n",
    "    # sanity check\n",
    "    assert len(pocket) == len(set([residue.res_id for residue in pocket_residues]))\n",
    "    # get centroid of pocket residues\n",
    "    return compute_centroid(pocket_residues)\n",
    "    \n",
    "def get_atom_array_from_pocket_center(pocket_centers):\n",
    "    array = AtomArray(len(pocket_centers))\n",
    "    for i, pocket_center in enumerate(pocket_centers):\n",
    "        array.coord[i] = pocket_center\n",
    "    return array\n",
    "\n",
    "def get_dcc_points(structure, holo_structures):\n",
    "    # load pocket centers from the dataset\n",
    "    pocket_centers = []\n",
    "    for holo_structure in holo_structures:\n",
    "        apo_pocket = holo_structure['apo_pocket_selection']\n",
    "        apo_pocket = [residue.split(\n",
    "            '_')[1] for residue in apo_pocket]\n",
    "        pocket_centers.append(compute_pocket_center(structure, apo_pocket))\n",
    "\n",
    "    pocket_centers = get_atom_array_from_pocket_center(pocket_centers)\n",
    "\n",
    "    # some pocket centers are the same/very close, let's cluster them\n",
    "    cluster_centers = compute_clusters( pocket_centers, np.array([1] * len(pocket_centers)))\n",
    "    clustered_centers = []\n",
    "    for cluster_id in range(-1, max(cluster_centers) + 1):\n",
    "        # if unclustered, then keep the original pocket center\n",
    "        if cluster_id == -1:\n",
    "            clustered_centers.extend(pocket_centers[cluster_centers == cluster_id].coord)\n",
    "        # else compute the centroid of the cluster\n",
    "        else:\n",
    "            this_cluster_centers = pocket_centers[cluster_centers == cluster_id]\n",
    "            # compute cluster centroid\n",
    "            cluster_centroid = compute_centroid(this_cluster_centers)\n",
    "            clustered_centers.append(cluster_centroid)\n",
    "\n",
    "    return np.array(clustered_centers)\n",
    "\n",
    "def count_correctly_predicted_pockets(points, clusters_centroids, metric='DCC'):\n",
    "    CUTOFF_THRESHOLD = 12 if metric == 'DCC' else 4\n",
    "\n",
    "    # Take only N cluster centroids with the highest average score, where N = len(points)\n",
    "    N = len(points)\n",
    "    # Sort clusters_centroids by average_score in descending order\n",
    "    clusters_centroids_sorted = sorted(clusters_centroids, key=lambda x: x[1], reverse=True)\n",
    "    # Select top N centroids\n",
    "    \n",
    "    top_clusters_centroids = [centroid for centroid, score in clusters_centroids_sorted[:N + K]]\n",
    "    # Compute the full distance matrix between all cluster centroids and all points\n",
    "\n",
    "    if len(top_clusters_centroids) == 0:\n",
    "        print(f\"No pockets found; N= {N}, len(clustered_centroids) = {len(clusters_centroids)}\")\n",
    "        return -1\n",
    "\n",
    "    distances = distance_matrix(points, top_clusters_centroids)\n",
    "\n",
    "    # loop over each actual pocket and check whether there is a predicted pocket that is close enough (<CUTOFF_THRESHOLD)\n",
    "    pockets_found = 0\n",
    "    for i, this_distances in enumerate(distances):\n",
    "        # get the closest predicted pocket\n",
    "        closest_pocket = np.argmin(this_distances)\n",
    "        # check if the distance is less than the cutoff threshold\n",
    "        if this_distances[closest_pocket] < CUTOFF_THRESHOLD:\n",
    "            pockets_found += 1\n",
    "    return pockets_found\n",
    "\n",
    "def analyze(metric='DCC'):\n",
    "    # set to 12.0 Angstroms for DCC, 4 Angstroms for DCA\n",
    "    # see https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00923-z\n",
    "    CUTOFF_THRESHOLD = 12 if metric == 'DCC' else 4\n",
    "\n",
    "    dataset = load_dataset()\n",
    "\n",
    "    total_pockets = 0\n",
    "    total_pockets_found = 0\n",
    "\n",
    "    for apo_structure, holo_structures in dataset.items():\n",
    "        chain_id = holo_structures[0]['apo_chain']\n",
    "\n",
    "        # skip multichain structures\n",
    "        if '-' in chain_id:\n",
    "            continue\n",
    "        \n",
    "        auth = load_structure(apo_structure)\n",
    "        auth = auth[\n",
    "                (auth.chain_id == chain_id) &\n",
    "                (biotite.structure.filter_peptide_backbone(auth))]\n",
    "\n",
    "        points = None\n",
    "        # load pocket centers\n",
    "        if metric == 'DCC':\n",
    "            points = get_dcc_points(auth, holo_structures)\n",
    "        \n",
    "        # load ligands\n",
    "        if metric == 'DCA':\n",
    "            raise NotImplementedError\n",
    "            ligands = []\n",
    "            for holo_structure in holo_structures:\n",
    "                # TODO: align to holo structure (https://www.biotite-python.org/latest/apidoc/biotite.structure.superimpose.html)\n",
    "                # TODO: use the AffineTransformation value to map the ligand to the apo structure\n",
    "                ...\n",
    "            points = np.array(ligands)\n",
    "\n",
    "        \n",
    "        protein_id = f'{apo_structure}{chain_id}'\n",
    "\n",
    "        # filter to get correct chain; filter only for peptides\n",
    "        auth_residues = auth[get_residue_starts(auth) + 1] # skip the first atom (N), second should be CA\n",
    " \n",
    "        # load predictions and cluster them\n",
    "        predictions = np.load(f'{PREDICTIONS_PATH}/predictions/{protein_id}.npy')\n",
    "        predictions_mask = predictions > THRESHOLD\n",
    "        assert len(predictions) == len(auth_residues), f'Length of predictions ({len(predictions)}) does not match length of auth residues ({len(auth_residues)})'\n",
    "        \n",
    "        # filter auth residues to get only the ones that are predicted to bind and cluster them into pockets\n",
    "        predicted_binding_residues = auth_residues[predictions_mask]\n",
    "        predicted_binding_scores = predictions[predictions_mask]\n",
    "        print(len(predicted_binding_residues))\n",
    "        clusters = compute_clusters(predicted_binding_residues, predictions[predictions_mask])\n",
    "\n",
    "        clusters_centroids = []\n",
    "        # loop over each cluster and compute the cluster's center with its average score\n",
    "        for cluster_id in range(-1, max(clusters) + 1):\n",
    "            if cluster_id == -1:\n",
    "                # skip unclustered residues\n",
    "                continue\n",
    "            cluster_residues = predicted_binding_residues[clusters == cluster_id]\n",
    "            # compute cluster centroid and compute the average score\n",
    "            cluster_centroid = compute_centroid(cluster_residues)\n",
    "            average_score = np.mean(predicted_binding_scores[clusters == cluster_id])\n",
    "            \n",
    "            clusters_centroids.append((cluster_centroid, average_score))\n",
    "\n",
    "        pockets_found = count_correctly_predicted_pockets(points, clusters_centroids, metric=metric)\n",
    "        if pockets_found == -1:\n",
    "            continue\n",
    "\n",
    "        total_pockets += len(points)\n",
    "        total_pockets_found += pockets_found\n",
    "\n",
    "    return total_pockets, total_pockets_found, total_pockets_found / total_pockets\n",
    "\n",
    "analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5ceed",
   "metadata": {},
   "source": [
    "## DCC\n",
    "Distance between the predicted and the real binding site center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe28ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DCC\n",
    "\n",
    "\n",
    "DATASET = 'cryptobench'\n",
    "CIF_FILES = '/home/vit/Projects/deeplife-project/data/cif_files'\n",
    "PREDICTIONS_PATH = '/home/vit/Projects/cryptic-nn/data/predictions/ESM2-3B-extended-finetuning'\n",
    "\n",
    "with open(f'../../datasets/{DATASET}-dataset/folds/test.json', 'r') as json_file:\n",
    "    dataset = json.load(json_file)\n",
    "\n",
    "skip = True\n",
    "for apo_structure, holo_structures in dataset.items():\n",
    "\n",
    "    # finished analysis at: '5wbmB' structure\n",
    "    if skip:\n",
    "        if apo_structure == '5wm9':\n",
    "            skip = False\n",
    "        else:\n",
    "            continue\n",
    "    # print(f'Processing {apo_structure} ...')\n",
    "    binding_residues = set()\n",
    "    chain_id = holo_structures[0]['apo_chain']\n",
    "\n",
    "    # skip multichain structures\n",
    "    if '-' in chain_id:\n",
    "        continue\n",
    "\n",
    "    for holo_structure in holo_structures:\n",
    "\n",
    "        apo_pocket = holo_structure['apo_pocket_selection']\n",
    "        \n",
    "        new_apo_residues = [residue.split(\n",
    "            '_')[1] for residue in apo_pocket]\n",
    "\n",
    "        binding_residues.update(new_apo_residues)\n",
    "\n",
    "    cif_file_path = rcsb.fetch(apo_structure, \"cif\", target_path=CIF_FILES)\n",
    "\n",
    "    cif_file = pdbx.CIFFile.read(cif_file_path)\n",
    "\n",
    "    auth = get_structure(cif_file, model=1)\n",
    "    auth = auth[\n",
    "            (auth.chain_id == chain_id) &\n",
    "            (biotite.structure.filter_peptide_backbone(auth))]\n",
    "    \n",
    "    protein_id = f'{apo_structure}{chain_id}'\n",
    "    # skip if no residues left\n",
    "    if len(auth) == 0: \n",
    "        print(f'No residues left for {protein_id}')\n",
    "        continue\n",
    "\n",
    "    # filter to get correct chain; filter only for peptides\n",
    "    auth_residues_only = get_residues(auth)\n",
    "\n",
    "    predictions = np.load(f'{PREDICTIONS_PATH}/predictions/{protein_id}.npy') > 0.5\n",
    "    \n",
    "    assert len(predictions) == len(auth_residues_only[0]), f\"Length of auth residues and predictions do not match for {protein_id}: {len(auth_residues_only[0])} vs {len(predictions)}\"\n",
    "    predicted_binding_residues = auth_residues_only[0][predictions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06ee37",
   "metadata": {},
   "source": [
    "## DCA\n",
    "Distance between the predicted binding site center and the closest ligand atom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39abe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
